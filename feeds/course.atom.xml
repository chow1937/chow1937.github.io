<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Tonychow's Blog - course</title><link href="https://blog.tonychow.me/" rel="alternate"></link><link href="https://blog.tonychow.me/feeds/course.atom.xml" rel="self"></link><id>https://blog.tonychow.me/</id><updated>2021-08-24T00:00:00+08:00</updated><entry><title>6.824 Lecture 4 Primary&amp;Backup Replication Notes &amp; Paper Reading</title><link href="https://blog.tonychow.me/mit6.824-letcture4-notes.html" rel="alternate"></link><published>2021-08-24T00:00:00+08:00</published><updated>2021-08-24T00:00:00+08:00</updated><author><name>tonychow</name></author><id>tag:blog.tonychow.me,2021-08-24:/mit6.824-letcture4-notes.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1 概要&lt;/h2&gt;
&lt;p&gt;本课主要是对分布式系统中复制这个主题进行了讨论，复制是为了容错而存在的，而复制本身又会带来更多的挑战。本课的论文是来自虚拟机提供商 VMWare 的 VMware vSphere Fault Tolerance 产品论文，这个产品是一个虚拟机指令级别的复制系统，是一个切实使用的企业级产品中的一个功能。虚拟机级别的复制对应用甚至操作系统都是透明的，VM 之上程序完全无感知，并且复制状态和粒度非常精确，可以带来很强大的复制功能，但是另一方面实现上也存在很多的困难。&lt;/p&gt;
&lt;p&gt;本课相关的材料:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;课堂 Paper - https://pdos.csail.mit.edu/6.824/papers/vm-ft.pdf&lt;/li&gt;
&lt;li&gt;课堂录像: https://youtu.be/gXiDmq1zDq4&lt;/li&gt;
&lt;li&gt;课堂 Note: https://pdos.csail.mit.edu/6.824/notes/l-vm-ft.txt&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2 要点&lt;/h2&gt;
&lt;h3&gt;2.1 异常与失效&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Fail-stop: 异常导致服务器宕机，完全无效 -&amp;gt; 网络、硬件、掉电等等;&lt;/li&gt;
&lt;li&gt;逻辑  bug: 分布式系统自身的代码逻辑错误导致异常；&lt;/li&gt;
&lt;li&gt;配置错误导致异常；&lt;/li&gt;
&lt;li&gt;不考虑恶意服务和黑客导致的异常；&lt;/li&gt;
&lt;li&gt;物理世界的异常：地震、台风等等导致机房出现异常或者网络中断；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.2 挑战&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;判断主节点 (Primary) 已经失效: 网络异常导致脑裂，备份节点认为主节点失效尝试提升为主节点并对外提供服务，在网络恢复后系统处于异常状态；&lt;/li&gt;
&lt;li&gt;维持主节点/备份节点数据同步：期待对于客户端无感，主备节点需要保证修改的顺序一致，并且处理好非确定性的执行结果同步；&lt;/li&gt;
&lt;li&gt;Fail-over : 异常出现时，备份节点怎么才可以正常成功接管对外服务？应该尽量减少对外的影响，外部无感知；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.3 复制同步方案&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;状态传输：定时创建状态的 checkpoint，然后同步到备份节点，实现上可以考虑对比上个 checkpoint，只传输差异；&lt;/li&gt;
&lt;li&gt;复制状态机 (RSM) : 主节点同步操作命令到备份节点，备份节点按顺序执行一致的操作，以保证主备状态一致；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目的都是为保证主备节点的状态一致，状态传输的方式在状态很大的情况下会比较耗时，并且对网络带宽要求很大。&lt;/p&gt;
&lt;p&gt;而复制状态机是常见的方法，上节课的 GFS 也是应用了复制状态机的方案。复制状态机的方案还需要考虑非确定的执行，比如获取当前时间的操作、或者随机数的操作。&lt;/p&gt;
&lt;p&gt;操作复制的级别&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;应用级别的操作: 比如 GFS 的文件追加写操作，和应用强相关;&lt;/li&gt;
&lt;li&gt;机器级别的操作指令: 机器执行的指令，与应用无关，不关注运行的应用程序；&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;2.4 VMware FT&lt;/h3&gt;
&lt;p&gt;虚拟机级别的复制，直接复制虚拟机执行指令操作，对应用程序透明，对请求的客户端隐藏具体实现，尽量减少复制对性能的影响。VMware FT 是真实的应用产品，是商用的。&lt;/p&gt;
&lt;h4&gt;2.4.1 总览&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;层级：硬件 -&amp;gt; 虚拟机 (VM / Hypervisor/VM FT) -&amp;gt; 操作系统(linux) -&amp;gt; 应用程序；&lt;/li&gt;
&lt;li&gt;捕获操作系统的中断，由 VM FT 执行复制到备份 VM，通过日志通道；&lt;/li&gt;
&lt;li&gt;主备之间存在一个日志通道用于传输主节点的非确定相关操作指令；&lt;/li&gt;
&lt;li&gt;客户端只与主节点交互和通信；&lt;/li&gt;
&lt;li&gt;备份节点和主节点执行一样的操作指令，但是不会直接与客户端交互，不会对外输出；&lt;/li&gt;
&lt;li&gt;备份节点操作系统的对外写数据会被 VM FT 处理掉；&lt;/li&gt;
&lt;li&gt;主备节点同网络内会存在一个存储服务，用于主备之间的协调处理和数据存储；&lt;/li&gt;
&lt;li&gt;主备节点之间如果存在网络分区，需要依赖共享存储去协调进行主备切换，备节点状态切换为单节点的主节点状态，并开始接收和处理客户端的请求及进行响应；&lt;/li&gt;
&lt;li&gt;主备节点在共享存储上利用一个类锁机制标识(flag)来实现状态切换；&lt;/li&gt;
&lt;li&gt;主节点失效，备份节点转换为主节点对外服务，这时候就是单点状态，需要考虑启动一个新的备份节点继续执行复制，VM FT 是利用 VMMotion 复制主虚拟机来实现创建新的备份虚拟机的；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;主节点通过日志通道发送给备份节点的情况：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;有可能会导致主备执行出现分离的时候，相关执行指令及数据会发送给备份节点：外部事件，客户端的 packet 数据，中断等；&lt;/li&gt;
&lt;li&gt;非确定性执行指令的结果需要发送给备份节点；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;备份节点必须要落后主节点一个日志条目的执行，以保证备份节点的执行不会超出主节点，导致和主节点出现分离。&lt;/p&gt;
&lt;h4&gt;2.4.2 设计&lt;/h4&gt;
&lt;p&gt;目标是希望对外能表现为一个单节点服务器一样，外部客户端对主备异常下的切换无感知。&lt;/p&gt;
&lt;p&gt;指令类型：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;确定性指令：inc, dec, ... 需要保证执行的顺序一致；&lt;/li&gt;
&lt;li&gt;非确定性指令：执行多次结果产生的结果不一样，如果直接复制到备份节点执行会影响到结果，导致主备的状态不一致 -&amp;gt; 获取时间操作，计时器中断&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;多核的情况：多个线程执行是抢占式的，难以保证主备的多核执行指令顺序一致，本论文中应用的是单核虚拟机，暂不考虑多核的实现。&lt;/p&gt;
&lt;p&gt;操作系统执行非确定指令时，操作被 FT 捕获，执行结果并记录结果，然后将非确定执行转换为确定性的执行，通过日志通道传输结果给备份节点，在备份节点执行同样的指令时也会捕获指令并返回一致的结果给备份节点的操作系统。&lt;/p&gt;
&lt;p&gt;主备复制机制的存在，为了避免在异常切换主备时对外输出存在不一致，主节点对外输出需要特别考虑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主节点执行输出前，需要确保产生输出的操作指令已经同步到备份节点；&lt;/li&gt;
&lt;li&gt;只是延迟对外输出，主节点接下来指令的执行还是继续的；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;客户端可以进行重试或者重置连接，但是基于输出规则，不会存在数据不一致。这个输出机制在实际应用时，比如流量大的应用，会导致应用带宽大幅度下降(40%)，因为主节点需要等待指令操作复制到背节点后再对外响应数据接收，所以运行时带宽就会下降。&lt;/p&gt;
&lt;h2&gt;3 Paper&lt;/h2&gt;
&lt;p&gt;本课讨论的论文是来自 VMware 公司的企业级商用系统  VMware vSphere Fault Tolerance，下文简称 FT。FT 包含在其商用产品  VMware vSphere 4.0 ，是一个虚拟机指令级别复制的容错系统，实现了主备虚拟机之间完整的指令及状态复制，应用性能影响不超过 10%，并且对运行的应用无需额外实现应用层的复制。&lt;/p&gt;
&lt;p&gt;我们常见的分布式系统中，为了容错而考虑复制机制时，通常采用的是应用级别的操作日志复制实现，而本论文的思路是对应用及操作系统运行的虚拟机级别指令进行复制。这个方案可以使得主备虚拟机之间无论上层运行的应用是什么，都可以完成无感透明的复制容错。&lt;/p&gt;
&lt;p&gt;这不是一个常见的方案，在具体实现上，涉及到虚拟机底层及硬件指令级别的诸多细节，可想而知存在着很多的难点。本论文主要描述了 FT 整体的设计思路及实现，其中详细讨论了几个关键点，但是整个系统的实现肯定是存在着大量的细节难点，甚至本论文实现的还是针对单核的虚拟机的复制。虽然这个方案并没有被广泛应用，但是 FT 实现中的一些实际考虑和设计也值得我们参考。&lt;/p&gt;
&lt;h3&gt;3.1 简介&lt;/h3&gt;
&lt;p&gt;当我们想要实现一个支持容错的分布式系统时，一个常见的方案是主备机制，备份服务始终保持和主服务的状态同步，这样在主服务挂掉时，备份服务可以直接接手对外提供一直的服务。主备服务之间状态的同步通常是通过复制实现的，而无论是什么级别的复制，我们实现一个主备复制支持容错的系统时有两种常见的方案：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;状态变化数据复制：把主节点的所有状态变化数据(CPU/Memory/IO设备等等)同步到备份节点；&lt;/li&gt;
&lt;li&gt;状态机复制：将主备节点视为启动时状态一致的状态机，这样只需要把会导致主节点状态变化的指令和数据同步到备份节点，然后由备份节点按和主节点一致的顺序执行，这样可以保证主备的状态完全一致，也就实现了同步的效果；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;状态变化数据复制需要对比状态差异，并且如果内存因为指令发生大量的变化，需要同步的数据将会是非常大的，而状态机复制是关注导致变化的指令，只需要同步指令，这样实际需要同步的数据就小很多。但是因为虚拟机有很多指令是非确定性的，每次执行的结果可能都不一致，这种指令需要额外的机制来协调处理，以保证主备节点之间的状态不会出现分离。&lt;/p&gt;
&lt;p&gt;在物理机器上实现指令级别的复制是很难的，特别是多核的服务器，各种非确定性指令难以实现同步。而本论文产品是基于 hypervisor 实现的虚拟机级别的指令复制，hypervisor 拥有对虚拟机执行指令的完全控制，可以捕获所有非确定性操作的必要信息，并且同步到备份虚拟机进行重放执行，保证状态一致。&lt;/p&gt;
&lt;p&gt;基于 hypervisor 实现的虚拟机状态机复制方法不需要硬件修改，可以直接运行在商用机器机器上，并且因为这个方案对于带宽要求不大，还可以考虑部署在不同地点服务器上，以保证更高的容错和可用性。&lt;/p&gt;
&lt;p&gt;本论文实现的 FT 是基于 x86 架构的，可以对操作系统及其上运行的应用提供透明的容错支持，实现上是依赖确定性指令回放 (deterministic replay) 的方式，当前版本只支持单处理器的虚拟机复制。&lt;/p&gt;
&lt;h3&gt;3.2 基本设计&lt;/h3&gt;
&lt;p&gt;&lt;img alt="vmft" src="../images/vmft.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如上图是 FT 的整体架构，主备 VM 分布部署在物理上分离的两个服务器上，只有主 VM  接收外部的请求和输入，并且通过 logging channel 把数据传输到备份 VM 上进行同步。除了外部输入数据之外，传输的还包含了非确定性的指令及相关信息会同步到备份 VM，备 VM 执行一致的指令和产生一致的结果，以保证主备 VM 状态安全一致。对于指令执行产生的对外输出，只有主 VM 的输出会响应到客户端，备份 VM 的输出将会被 hypervisor 丢弃。&lt;/p&gt;
&lt;h4&gt;3.2.1 确定性回放实现&lt;/h4&gt;
&lt;p&gt;FT 复制是基于确定性状态机复制来实现的，如果两个状态机初始状态一致，那么只要保证输入数据和顺序完全一致，这两个状态机就可以保证一致的状态变化，产生一样的输出。&lt;/p&gt;
&lt;p&gt;VM 的输入包含以下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;网络数据包&lt;/li&gt;
&lt;li&gt;磁盘读&lt;/li&gt;
&lt;li&gt;键盘及鼠标等外设的输入信息&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;非确定性的事件(虚拟中断)及操作(读取处理器的时钟)也会影响到 VM 的状态，对于实现 VM 的执行复制，主要是面临着下面三个挑战：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;正确捕获所有的输入和非确定性操作；&lt;/li&gt;
&lt;li&gt;正确地把输入和非确定操作应用到备份 VM；&lt;/li&gt;
&lt;li&gt;完成以上两点并且不会降低整体的性能，或者尽量要减少影响；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;实现上，一些复杂的操作系统在 x86 处理器上也有一些未定义的指令执行副作用，对于这些的捕获和在备份 VM 上重放也是一个额外的挑战。&lt;/p&gt;
&lt;p&gt;VMware 的 FT 在限定的范围内解决了上面提的这些挑战，其中的确定性回放机制记录了 VM 的输入和关联的非确定性操作指令，以日志条目的形式写到日志文件，再同步到备份 VM 完全按照顺序执行，产生一致的结果和状态。&lt;/p&gt;
&lt;p&gt;对于非确定性的指令操作，需要记录主 VM 执行时的额外信息同步到备份 VM，比如 timer 或者 IO 完成中断，这种非确定性事件发生时的指令也会被记录下来，在备份 VM 重放时，执行到该记录指令时，也会产生同样的中断事件。&lt;/p&gt;
&lt;h4&gt;3.2.2 FT 协议&lt;/h4&gt;
&lt;p&gt;FT 中使用确定性回放来产生相关的操作日志，但是日志记录还需要通过日志通道传输到备份 VM 执行，为了确保实现容错支持，FT 对日志通道和传输提出了一个协议 (个人感觉应该是成为原则) ，其中基本的需求是：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输出需求：备份 VM 在主 VM 失效接管后，需要以保证和主 VM 已经发送到外部的所有输出完全一致的方式继续执行；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;对于一个基于复制机制实现高可用的分布式系统来说，高可用不仅要求节点异常时其他节点可以进行故障切换接管对外服务，还需要实现对于外部客户端来说，整个故障切换过程是无感知的。而系统对外的沟通就是输出，所以这里只要能保证主备切换时对外输出是一致的，那么对于外部客户端来说，整个切换就是无感知的。&lt;/p&gt;
&lt;p&gt;在这里，FT 提出了一个规则：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;输出规则：主 VM 在备份 VM 接收并且确认产生输出的关联指令之前都不能发送输出数据到外部；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从这个输出规则来看，只要备份 VM 接收到了产生输出的所有关联指令，包含输出的那个指令，那在主 VM 失效时，备份 VM 是可以顺利接管并且和之前输出保持一致对外继续产生输出。如果相关的指令没有接收到就接管对外服务，因为主 VM 并未对外产生输出，那么接下来的输出就由备份 VM 产生了，即使执行产生了非确定的事件，对于外部客户端来说整个输出也是一致的。&lt;/p&gt;
&lt;p&gt;值得注意的一点是，输出规则并不会限制主 VM 继续执行其他的指令，只是延迟了主 VM 对外部输出的命令。因为输出相关基本也是 IO 操作，而操作系统可以支持非阻塞的网络 IO 和磁盘输出，并且通过异步中断来指示输出完成，所以延迟对外部输出对于操作系统执行其他的指令并不会产生太大的影响。&lt;/p&gt;
&lt;p&gt;&lt;img alt="ftprotocol.jpeg" src="../images/ftprotocol.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;如上图是一个示例，从左到右是主备节点按时间顺序执行指令，而主备之间的线是同步相关的日志，其中主 VM 往备份 VM 同步日志，备份 VM 向主 VM 响应收到日志，上图相关的执行顺序如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;异步事件从主 VM 同步到备份 VM；&lt;/li&gt;
&lt;li&gt;输入指令及数据从主 VM 同步到备份 VM；&lt;/li&gt;
&lt;li&gt;主 VM 输出操作同步到备份 VM ，但是这时主 VM 并未对发生输出，而且暂时延迟了；&lt;/li&gt;
&lt;li&gt;备份 VM 响应收到输出相关的指令给主 VM，此时主 VM 可以执行真正的对外输出；&lt;/li&gt;
&lt;li&gt;主 VM 发生异常失效时，备份 VM 进行故障切换，接管对外服务，因为输出相关指令已经接收到，备份 VM 可以和主 VM 一致地对外输出；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;此外，FT 并不保证对外输出是仅有一次的，比如主 VM 接收到备份 VM 的输出相关日志确认后，对外进行输出，然后出现异常，备份 VM 接管后也可能会再次对外进行输出。这种情况，需要依赖操作系统及传输协议来进行容错，比如 TCP 协议对于重复的数据包会自动进行识别和丢弃，这样对于应用来说也是无感知的。&lt;/p&gt;
&lt;h4&gt;3.2.3 检测和响应故障&lt;/h4&gt;
&lt;p&gt;在实际运行的过程中，主 VM 和备份 VM 都有可能发生故障：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;备份 VM 故障时：主 VM 需要从复制日志的模式切换到普通执行模式，停止往日志通道发生日志记录；&lt;/li&gt;
&lt;li&gt;主 VM 故障时：备份 VM 接管，但是需要先执行完毕所有从主 VM 同步并且已经确认的指令，然后就从回放模式转换为普通执行模式，并且被提升为主 VM，被允许对外产生输出；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;备份 VM 接管时，系统会自动广播新的主 VM  MAC 地址到网络中，这样物理网络设备可以识别主 VM 的位置。&lt;/p&gt;
&lt;p&gt;故障的检测有下面几点相关的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;FT 系统会通过 UDP 心跳来检测运行 VM 的服务器的状态；&lt;/li&gt;
&lt;li&gt;FT 还会检测日志通道的流量情况，包括主向备发送的日志和备向主发生的确认，因为操作系统总是存在定期的 timer 中断，所以主备之间的流量是不会完全消失的；&lt;/li&gt;
&lt;li&gt;网络分区总是可能存在的，如果只按上面两点，是有可能存在脑裂的情况，备份 VM 以为主 VM 发生故障而自提升为主 VM，同时存在两个主 VM；&lt;/li&gt;
&lt;li&gt;FT 中主要是利用共享的存储来处理脑裂的情况，主备 VM 尝试接管对外服务时，会在共享存储上执行一个原子的 test-and-set 操作，如果操作成功就会被允许继续，失败则停止；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;最后，当 FT 出现一个 VM 故障时，在另外一个 VM 成功接管后，FT 会再启动一个冗余的备份 VM，继续进行复制和维持与主 VM 的状态一致。&lt;/p&gt;
&lt;h3&gt;3.3 FT 的实践设计&lt;/h3&gt;
&lt;h4&gt;3.3.1 VM 的启动与重启&lt;/h4&gt;
&lt;p&gt;在 FT 中的一个 VM 启动或者重启时，能够快速地将 VM 的状态和已有 VM 同步是一个很关键的问题，并且我们希望这个 VM 的启动不会影响到现有 VM 的性能和执行。在 FT，这主要是通过使用 VMware VMotion 功能修改版本来实现的。VMotion 是设计用于实现将 VM 在不同服务器上迁移的，在 FT 中，具体实现调整为复制，创建日志通道并且让主 VM 进入日志记录模式，并且尽量减少影响，整个过程不会导致主 VM 中断超过一秒。&lt;/p&gt;
&lt;p&gt;FT 中的所有 VM 都是运行在同一个 VMware vSphere 集群服务器上的，共享了同样的存储，所以在 VM 需要启动一个新的备份 VM 时，可以由集群管理服务根据资源和情况选择一个合适的服务器进行复制和启动，整个过程完全自动并且外部无感知。&lt;/p&gt;
&lt;h4&gt;3.3.2 日志通道管理&lt;/h4&gt;
&lt;p&gt;日志通道相关的实现上，FT 系统是通过 hypervisor 在主备 VM 上都维持一个很大的缓存空间，主 VM 把需要同步的操作日志写到主 VM 的缓存空间上，备份 VM 从其缓存空间消费和处理执行操作日志。日志被写到主 VM 的缓存空间后立即就会备刷入到日志通道上，并且尽快地读取到备份 VM 的日志缓存上。日志从网络到备份 VM 的日志缓存后就会立即给主 VM 发生日志确认消息，以方便主 VM 执行输出规则相关逻辑。&lt;/p&gt;
&lt;p&gt;下面几个情形会影响整体的执行：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;主 VM 的日志缓存被写满：主 VM 会停止执行，这将会影响到外部客户端；&lt;/li&gt;
&lt;li&gt;备份 VM 的日志缓存被写满：备份 VM 会进入停止状态，因为备份 VM 本来就不对外输出，所以对外部无影响；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;主 VM 的日志缓存是有可能被写满的，虽然备份 VM 理论上执行速度应该是和主 VM 基本一致的，但是在极端压力的情况下，备份 VM 运行的服务器上可能会存在资源不足，包括 CPU， 内存等资源可能都缺乏，从而导致了相关操作的执行慢于主 VM，随着运行时间增长，主备之间的差距可能会越来越大，也就有可能导致主 VM 的日志缓存被写满。&lt;/p&gt;
&lt;p&gt;从 FT 系统的实现来考虑，我们也不希望主备 VM 之间的指令间隔越来越大。因为在主 VM 失效之后，备份 VM 需要接管对外进行服务，在正式对外输出之前，备份 VM 也需要将同步到的日志按顺序执行完毕才行。如果间隔过大，那在备份 VM 在能正式对外提供服务之前会有比较大的一段时间，这样外部也会感知到了 FT 下的主备 VM 异常。&lt;/p&gt;
&lt;p&gt;基于降低 FT 中主备 VM 操作日志间隔的考虑，FT 对于这种异常情况的处理方案是，通过额外的信息记录主备 VM 之间的执行间隔和保证大小 (100 毫秒)。当执行间隔大于一定值时 (大于 1 秒)，主 VM 的执行速度将会被减缓，直到主备 VM 之间的执行间隔恢复到正常范围，如果备份 VM 的执行速度跟上来，主 VM 的 CPU 限制也会被逐渐放开，整体的执行性能也慢慢恢复到正常值。&lt;/p&gt;
&lt;h4&gt;3.3.3 FT VM 运维&lt;/h4&gt;
&lt;p&gt;对于 FT 中的 VM 正常运维操作也需要进行考虑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主 VM 执行正常关机操作，备份 VM 也应该进行关机，而不应该尝试接管对外服务；&lt;/li&gt;
&lt;li&gt;主 VM 的资源比如 CPU、内存进行了调整，备份 VM 也应该执行相对应的调整；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些运维操作，会生成特定的控制日志并发生到日志通道上传输给备份 VM 执行。对于大部分针对 VM 的运维操作，一个原则是这些操作只能在主 VM 上执行，然后再通过特定的控制日志同步给备份 VM 执行。&lt;/p&gt;
&lt;p&gt;唯一例外的运维操作是 VMotion 的执行，这个操作可以针对备份 VM 进行，用于调整备份 VM 的运行服务器或者恢复备份 VM。VMotion 操作给 FT 的实现带来很大的复杂性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主 VM 进行 VMotion 操作时，备份 VM 需要断开和原来主 VM 的日志通道连接，再重新连接到新的主 VM；&lt;/li&gt;
&lt;li&gt;备份 VM 进行 VMotion 时，也需要主 VM 进行日志通道的切换；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在进行 VMotion 操作时，FT 要求所有的磁盘操作都完全暂停，对于主 VM 来说，磁盘操作暂停很容易，直接执行就可以了，而备份 VM 则面临更复杂的情况。因为备份 VM 是执行主 VM 的指令的，这也可能包含正常的磁盘 IO 操作，对于备份 VM 来说难以保证在正确的地方停止磁盘 IO。实现上，当备份 VM 在 VMotion  操作最后切换的阶段时，会通过日志通道通知主 VM 临时停止所有的 IO 操作，这样备份 VM 消费到主 VM 同步过来的 IO 停止后会按照顺序执行，然后再执行后续的正常操作。&lt;/p&gt;
&lt;h4&gt;3.3.4 磁盘 IO 相关实现问题&lt;/h4&gt;
&lt;p&gt;在 FT 实现上，关于磁盘 IO 有几类问题需要考虑和解决。&lt;/p&gt;
&lt;p&gt;第一是磁盘 IO 操作的非确定性执行。磁盘 IO 操作是非阻塞并且可以并行执行访问同一个磁盘位置的，并且在使用 DMA 操作进行内存和磁盘之间的数据传输时，也可能会并发访问同一个内存位置，而并发带来不确定性。FT 主要是通过检测这类型的 IO 操作并转换为顺序的执行来解决这个问题的。&lt;/p&gt;
&lt;p&gt;第二个问题是磁盘操作和应用内存操作之间的冲突。磁盘操作包含了 DMA 可以直接读取内存数据，而当应用和磁盘操作同时访问内存同样位置时，也会带来不确定性。FT 主要是通过一个临时的弹性缓存 (bounce buffer) 来解决这个问题的。这个弹性缓存空间和磁盘操作要访问的内存大小一致，磁盘读操作的数据将会先缓存到这个弹性缓存空间中，直到读 IO 完成才会将数据拷贝到操作系统内存。而写磁盘操作则先写到弹性缓存空间，然后再从弹性缓存写到磁盘上。&lt;/p&gt;
&lt;p&gt;第三个问题是主 VM 的磁盘操作可能在主 VM 发生异常失效时还没有完成，对于即将要被提升为主 VM 的原备份 VM 来说这个是不可知结果的，并且也不会接收到相关的磁盘 IO 操作完成事件。新的主 VM 也不能直接对操作系统返回一个磁盘 IO 操作失败，因为这样可能会导致不确定的情况，操作系统未必能正常处理磁盘 IO 错误。FT 的方法是重新执行等待完成事件的磁盘 IO 操作，配合之前并发 IO 调整为顺序执行，这样可以保证重新执行也不会产生异常的不一致的情况。&lt;/p&gt;
&lt;h4&gt;3.3.5 网络 IO 相关实现问题&lt;/h4&gt;
&lt;p&gt;VMware vSphere 对于 VM 的网络相关内容有特别的性能优化，主要是基于 hypervisor 的异步更新虚拟机的网络设备来实现的。但是异步操作往往意味着非确定性，对于 FT 的 VM 来说，很有可能会导致主备 VM 的状态发生分离。所以，在 FT 中，异步网络优化被取消了，异步地将接受到的数据包更新到 VM  ring buffer 的代码会被 hypervisor 拦截并记录下来再应用到 VM 上。&lt;/p&gt;
&lt;p&gt;取消网络的异步优化对于性能有一定的影响，在 FT 的实现上，主要是采用以下两个方法来提升 VM 的网络性能：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于集群优化 VM 的相关拦截和中断：hypervisor 在流式数据传输时可以按数据包分组进行拦截处理，同样相关的中断也可以分组批量进行；&lt;/li&gt;
&lt;li&gt;减少数据包传输的延迟：如上面提及的，基于输出原则，主 VM 需要延迟对外的数据传输直至备份 VM 确认相关联的日志已经收到，hypervisor 支持在 TCP 协议栈上注册函数，在接收到数据时可以从一个延迟执行的上下文调用，这样备份 VM 在接收到日志和主 VM 接收到确认都可以快速处理，而无需进行上下文切换。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;3.4 参考设计&lt;/h3&gt;
&lt;p&gt;本节内容主要是一些实现方案的参考设计方案。&lt;/p&gt;
&lt;h4&gt;3.4.1 共享和非共享磁盘&lt;/h4&gt;
&lt;p&gt;FT 的默认设计实现中，主备 VM 是共享相同的虚拟磁盘的，这样两个 VM 之间的数据天然就是保持一致，只要维持只有主 VM 对外输出，在故障切换时，备份 VM 可以直接在同样的虚拟磁盘上进行读写操作。&lt;/p&gt;
&lt;p&gt;另外一个方案是主备 VM 使用分离的磁盘，各自维护各自的磁盘数据状态，备份 VM 也需要执行写磁盘相关操作。这种方案在主备 VM 物理上的距离太长而不能共享虚拟磁盘的情况，此外对于无法使用共享磁盘的场景也可以支持。非共享磁盘的方案有几点需要注意的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;因为主备 VM 各自维护分离的磁盘，所以对于磁盘写，主 VM 不需要根据输出规则进行延迟；&lt;/li&gt;
&lt;li&gt;首次启用主备 VM 时，需要把主 VM 磁盘数据快速同步到备份 VM 的磁盘；&lt;/li&gt;
&lt;li&gt;主备可能发生不同步的情况，在同步恢复之后还需要额外考虑磁盘数据的同步处理；&lt;/li&gt;
&lt;li&gt;对于脑裂的场景，因为不存在共享磁盘来处理，主备 VM  需要依赖额外的第三方来进行协调；&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;3.4.2 备份 VM 执行磁盘读操作&lt;/h4&gt;
&lt;p&gt;FT 的默认设计中，备份 VM 不会从其虚拟磁盘中读数据，只会从主 VM 读取后再视为输入操作同步日志到备份 VM 执行进行同步。&lt;/p&gt;
&lt;p&gt;一个替代方案是支持备份 VM 读取磁盘数据，这样主 VM 就不再需要同步磁盘读取的数据，这样可以降低整个日志通道的带宽和数据。不过这个方案也有几点问题：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先因为备份 VM 也需要执行磁盘读操作，而磁盘读操作有可能会延迟，备份 VM 的执行速度将有可能受到影响，从而影响到主备 VM 的同步处理，此外主 VM 磁盘操作完成后，备份 VM 有可能还没完成；&lt;/li&gt;
&lt;li&gt;备份 VM 读取磁盘的操作是有可能失败的，这样还需要进行额外的处理，比如主 VM 读取磁盘成功但是备份 VM 读取失败的情况，或者反过来，实现上都需要更复杂的处理；&lt;/li&gt;
&lt;li&gt;在共享磁盘的方案下，如果主 VM  对磁盘的同样位置进行读写，那为了让备份 VM 后续同步后能读取到同样的数据，还需要考虑延迟主 VM 的写磁盘操作。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;总而言之，支持备份 VM 也执行磁盘读操作会带来实现上很大的复杂性，而优点只是降低日志通道的带宽和流量。这个方案只是在特定的应用场景下适用，比如磁盘读操作非常多而写相对少的应用。&lt;/p&gt;
&lt;h2&gt;4 总结&lt;/h2&gt;
&lt;p&gt;VMware 的 FT 系统是非常有意思的一个实现，在看到本篇论文之前，个人对于复制相关的系统基本上都是基于应用层的一个理解和场景印象，而 FT 基于虚拟机指令级别的复制的确给我带来的一定的震撼。当然，FT 的实现还是有其前提条件的，一个是基于 hypervisor 对虚拟机的管理，使得 FT 系统可以拦截和处理虚拟机的任何执行指令，进行复制相关的逻辑，另外，本论文实现的 FT 还是只针对单处理器的虚拟机，对于并行多核的虚拟机暂未有实际的支持和实现。&lt;/p&gt;
&lt;p&gt;虽然 FT 的应用和实现比较受限，但是最终还是实现了一个完整商用的虚拟机级别的复制容错分布式系统，在 FT 上运行的任意操作系统及应用，都可以无需任何额外处理即可获得分布式容错的能力。而 FT 实现中的很多设计和考虑，在我们实现应用层级别的复制时也是可以进行参考的。&lt;/p&gt;</content><category term="mit6.824"></category><category term="distributed-system"></category></entry><entry><title>6.824 Lecture 3 GFS Notes &amp; Paper Reading</title><link href="https://blog.tonychow.me/mit6.824-letcture3-notes.html" rel="alternate"></link><published>2021-08-02T00:00:00+08:00</published><updated>2021-08-02T00:00:00+08:00</updated><author><name>tonychow</name></author><id>tag:blog.tonychow.me,2021-08-02:/mit6.824-letcture3-notes.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1 概要&lt;/h2&gt;
&lt;p&gt;本课主要是针对分布式系统中容错这个主题进行了讨论，关注的领域是分布式存储系统。先是概述了分布式存储系统的关键点和挑战，然后围绕 GFS 的实现进入了更深入的探讨。GFS 是一个曾经在谷歌中大规模应用的真实分布式系统，对之前课程中涉及到的 MapReduce 应用提供了底层的文件系统支撑。&lt;/p&gt;
&lt;p&gt;本课相关的材料:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;课堂 Paper - GFS: https://pdos.csail.mit.edu/6.824/papers/gfs.pdf&lt;/li&gt;
&lt;li&gt;课堂录像: https://youtu.be/6ETFk1-53qU&lt;/li&gt;
&lt;li&gt;课堂 Note: https://pdos.csail.mit.edu/6.824/notes/l-gfs.txt&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;PS. 从课程官网发现 2021 学年新的课程录像已经同步更新到了 Youtube 上，因为疫情原因，课程是线上授课的形式。和 2020 年度现场授课的课程录像相比，线上授课可以直接打开论文进行讨论，整体的信息更丰富一点。所以从第 3 课开始，切换为 2021 年的课程录像进行学习，之前的两课就不再更新了。此外，课程安排相对旧学年有所调整。&lt;/p&gt;
&lt;h2&gt;2 要点&lt;/h2&gt;
&lt;h3&gt;2.1 存储系统&lt;/h3&gt;
&lt;p&gt;构建容错的分布式存储系统是分布式领域的一个关键领域:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;分布式存储保持全局的持久状态&lt;/li&gt;
&lt;li&gt;应用可以基于分布式存储无状态部署和运行&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.1.1 分布式存储为什么困难?&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;高性能 -&amp;gt; 数据分片(多服务器)，提升系统吞吐量&lt;/li&gt;
&lt;li&gt;大量的服务器 -&amp;gt; 错误是常态: 一台计算机一年出现一次错误，一千台服务器，每天都可能会出现错误&lt;/li&gt;
&lt;li&gt;容错设计在大型分布式系统中是必须考虑的 -&amp;gt; 复制&lt;/li&gt;
&lt;li&gt;复制 -&amp;gt; 数据同步问题(可能存在潜在的不一致)&lt;/li&gt;
&lt;li&gt;强一致性 -&amp;gt;  一致性协议 -&amp;gt; 复杂消息交互和传输 -&amp;gt; 性能降低&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;在系统引入了复杂性之后，往往会带来另外一个问题，复杂系统就是需要不断地解决不同的问题，然后在实现时对不可能的点做权衡取舍。&lt;/p&gt;
&lt;h4&gt;2.1.2 分布式系统中的一致性 (high level)&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;理想的一致性: 就和单服务器表现一样 (完全隐藏背后的大量服务器和复杂交互)&lt;/li&gt;
&lt;li&gt;服务器在并发时也能逐次执行客户端操作&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;读取到的数据是最近写的数据&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;并发: 单机在应对大量客户端请求时，也需要处理好并发&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为了容错引入复制机制，让分布式系统实现强一致性更困难，对于复制协议的实现需要更多的考虑。而复制协议的选择需要考虑实际系统的需求和真实的业务场景。&lt;/p&gt;
&lt;h3&gt;2.2 GFS&lt;/h3&gt;
&lt;p&gt;本课的论文是 Google File System，在谷歌当年广泛应用在各种大数据应用(MapReduce, 爬虫, 日志存储分析)中的底层文件系统。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;高性能: 复制+容错+某种程度的一致性&lt;/li&gt;
&lt;li&gt;成功的系统: 在谷歌内部广泛应用，MapReduce 的底层文件系统，成千台的服务器&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.2.1 Non-standard design&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;单 Master: 存在单点故障问题&lt;/li&gt;
&lt;li&gt;存在不一致性: 弱一致性实现&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GFS 的实现不是一个完美实现分布式算法或者理论的标准分布式系统，它是一个谷歌基于自身实际业务特征实现的一个可大规模部署应用的成功的分布式系统。&lt;/p&gt;
&lt;h4&gt;2.2.2 特点&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;大规模: 大量的数据集&lt;/li&gt;
&lt;li&gt;文件自动分片&lt;/li&gt;
&lt;li&gt;全局性: 上千台存储服务器对于所有应用代码来说都是同一个文件系统&lt;/li&gt;
&lt;li&gt;容错: 错误比如会出现，容错及自动恢复&lt;/li&gt;
&lt;li&gt;业界应用的真实大型分布式系统&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.2.3 设计&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;应用: 类似 MapReduce 中的 Map 或者 Reduce 任务，作为 GFS 的客户端&lt;/li&gt;
&lt;li&gt;Master: 应用与 Master 进行通信执行创建、打开、写入文件操作 -&amp;gt; chunk locations&lt;/li&gt;
&lt;li&gt;Chunk: 64 MB，可以多副本&lt;/li&gt;
&lt;li&gt;读写: 应用直接与 ChunkServer 通信 -&amp;gt; 系统吞吐量可以很大，多个应用可以并发操作访问&lt;/li&gt;
&lt;li&gt;ChunkServer: 文件没有特殊格式，就是以 64MB 保存到 Linux 文件系统中&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.2.4 Master 状态数据&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;filename -&amp;gt; chunk handles 数组: 一个文件可以由多个 chunk 组成&lt;/li&gt;
&lt;li&gt;Chunk handle: 版本号+chunkserver 列表(primary + secondaries)+租约(lease)&lt;/li&gt;
&lt;li&gt;Log + Checkpoint : 操作日志及检查点数据&lt;/li&gt;
&lt;li&gt;Log -&amp;gt; 持久存储: 操作先写入到 Log&lt;/li&gt;
&lt;li&gt;Checkpoint -&amp;gt; 持久存储: Master 内存数据的快照，方便重启时快速启动&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;单点的 Master 受限于单服务器资源限制，整个 GFS 系统能容纳的文件是存在一个上限的，而且根据后面谷歌工程师的访谈记录，这个限制在谷歌实际应用时，随着业务的发展和数据量的扩大，的确达到了，成为了一个瓶颈。谷歌后来也开始实现了多 Master 的类似系统来优化这个系统。&lt;/p&gt;
&lt;h4&gt;2.2.5 读文件&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;客户端向 Master 发送读请求，带上文件名和 offset&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Master -&amp;gt; 客户端: chunk handle + chunk servers + version number&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;客户端: 缓存 Master 返回的数据 -&amp;gt; 降低对 Master 的压力&lt;/li&gt;
&lt;li&gt;客户端从最近的 chunk server 读取 -&amp;gt; 减少网络传输时间及降低集群网络流量&lt;/li&gt;
&lt;li&gt;chunk server 检查 version number -&amp;gt; ok， 发生数据，避免读取到旧数据&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;2.2.6 写文件&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Append 是常见的操作场景: MapReduce 场景，Map 写中间文件和 Reduce 写最终文件都是 Append 操作&lt;/li&gt;
&lt;li&gt;写操作需要考虑 chunk 是否有 primary&lt;/li&gt;
&lt;li&gt;如果没有 primary 需要提升一个 secondary 为 primary ，并且增加 verison number&lt;/li&gt;
&lt;li&gt;version number 必须保存在持久存储中：恢复时需要读取&lt;/li&gt;
&lt;li&gt;客户端可以从 Master 拿到该 chunk 的所有 Primary 及 Secondary 节点信息&lt;/li&gt;
&lt;li&gt;数据先从客户端写到该 chunk 所有节点: pipeline 的方式，流水线传输数据&lt;/li&gt;
&lt;li&gt;Master 需要检测客户端写操作的 version number + lease&lt;/li&gt;
&lt;li&gt;存在一个 secondary 写失败，会返回错误给客户端，客户端需要重新尝试写操作&lt;/li&gt;
&lt;li&gt;at least once&lt;/li&gt;
&lt;li&gt;Chunk 的多个节点之间的数据文件可能不一致&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3 Paper&lt;/h2&gt;
&lt;p&gt;本课的论文是来自谷歌的 GFS，即 Google File System，发表于 2003 年。GFS 在谷歌的服务器集群中是作为 MapReduce 框架等大数据应用的底层分布式文件系统，运行于大量的廉价商用服务器上，并且实现了容错机制，对于大数据任务提供了很不错的性能。GFS 对业界分布式系统设计和实现影响很大，Hadoop 生态中的 HDFS 就是它的开源实现版本。&lt;/p&gt;
&lt;h3&gt;3.1 简介&lt;/h3&gt;
&lt;p&gt;GFS 的设计和实现来自谷歌中对于大数据处理急剧增长的需求，是一个基于实际业务需求而设计的分布式系统。实现上，除了考虑传统分布式系统中的扩展性、鲁棒性和可用性等常见特性之外，还根据谷歌的应用实际负载模式和技术环境有额外的考虑：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;组件失效是常态而不是异常：硬件是廉价的商用服务器，各种组件都很可能出错，应用 bug、操作系统 bug、磁盘、内存、连接器、网络，甚至电源都会出现问题；&lt;/li&gt;
&lt;li&gt;要处理的文件很大：GB 级别，对于当年常见的数据文件来说是非常巨大的；&lt;/li&gt;
&lt;li&gt;大部分文件写是追加写：这个是基于类似 MapReduce 这类型应用来说的，并且文件内容一旦写入了，比较少更新，大部分情况下是读取文件内容进行处理，这就说明 GFS 不是用于替换操作文件系统的；&lt;/li&gt;
&lt;li&gt;系统 API 设计上是基于应用实际需求来进行的，比如对应用提供的原子 Append 操作支持，在谷歌的应用常见下就非常有用；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;GFS 不是设计用来作为操作系统基础的文件系统的，所以并没有提供 POSIX 兼容的文件系统 API。&lt;/p&gt;
&lt;h3&gt;3.2 设计与实现&lt;/h3&gt;
&lt;h4&gt;3.2.1 假设&lt;/h4&gt;
&lt;p&gt;GFS 的设计是基于下面这些假设来进行的，在设计一个系统时，先对系统的应用场景进行假设和限制，这样我们最终实现的系统才是真正需要的系统：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;系统是在廉价的商业服务器上构建的，服务器经常会失效， 这意味着系统必须要时刻检测自身的节点状态，实现容错和快速及时地从错误恢复；&lt;/li&gt;
&lt;li&gt;系统存储的是大量的大文件，可能会有上百万个 100 MB 甚至更大的文件，几个 GB 大小的文件也是常见的，小文件可以支持，但是不会考虑特别的优化；&lt;/li&gt;
&lt;li&gt;工作负载常见是两种读方式：数据量大的流式读和小数据量的随机读，流式读通常是每个操作几百 KB 或者 1MB 的数据量，同一个客户端一般会顺序读取一个文件的内容，随机读则是在文件的任意 offset 读取几 KB 的数据，对于关注性能的应用来说，通常是将随机读批量打包为顺序往前读取的操作，而不是前后移动；&lt;/li&gt;
&lt;li&gt;写方式则常见是大量顺序的追加写文件，操作大小和流式读差不多，并且一旦写入文件内容，基本是不再进行修改，小数据量的随机写也支持，但是不会做特别的优化，效率不会很高；&lt;/li&gt;
&lt;li&gt;对于多客户端并发写入到同一个文件的场景，系统必须高效地实现相关的并发语义支持。常见的应用方式是基于文件做生产者和消费者队列的方式，数百个分布在数百个服务器的生产者并发追加写入到一个文件，并发操作的原子同步实现尽量要最少的额外开销，这是很关键的一点；文件读取可能是滞后的，也可能是和写同时进行的；&lt;/li&gt;
&lt;li&gt;对于系统来说，高吞吐量要比低延迟更重要，系统支持的应用对于数据处理的吞吐量需求要高于处理延迟，批量大数据处理才是整个系统的目标；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总结下来，GFS 应该是一个支持容错、支持大量大文件和超大文件(GB 级别)存储、特别对顺序流式写和读优化、支持并发追加写和关注整体系统吞吐量的一个分布式存储系统。对于上面的这些假设或者说需求，GFS 是怎么实现的呢？下面是详细的一个架构和交互设计。&lt;/p&gt;
&lt;h4&gt;3.2.2 接口&lt;/h4&gt;
&lt;p&gt;GFS 虽然没有实现一个标准的 POSIX 文件系统 API，但是也提供了和常见文件系统非常相似的接口。文件也是层级组织的目录，并且根据路径名标识，支持常见的 create, delete, open, close, read 和 write 操作接口。&lt;/p&gt;
&lt;p&gt;除了常见的文件操作之外，GFS 还支持两个比较独特的操作， snapshot 和 record append 。snapshot 操作主要是用来对数据进行备份，支持对目录树或者文件进行低成本的拷贝，实现上是基于 copy on write 的。而 record append 则是特别针对 GFS 的常见应用场景和需求而实现提供的，支持多客户端原子并发写入到同一个文件。&lt;/p&gt;
&lt;h4&gt;3.2.3 整体架构&lt;/h4&gt;
&lt;p&gt;&lt;img alt="gfs.jpeg" src="../images/gfs.jpeg"&gt;&lt;/p&gt;
&lt;p&gt;上图是 GFS 整体的一个架构，包含了主要的交互节点角色及部分的数据流转和控制交互。在 GFS 中，关键的节点和组件如下：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Master：单节点部署，保存整个 GFS 集群中所有文件的元数据，包括文件命名空间、访问控制信息、文件到 chunk 列表的映射关系、每个 chunk 保存的 chunkserver 位置等；还会进行整个系统基本的操作处理，比如 chunk 的租约信息管理、孤儿 chunk 的垃圾回收检索处理、chunk 在 chunkserver 之间的迁移、复制，并且还会和每个 chunkserver 定时有心跳消息，下发指令及采集状态；&lt;/li&gt;
&lt;li&gt;Chunk：文件被划分为多个 chunk，每个 chunk 是固定大小的一个文件，由 Master 分配的一个不可变且唯一的 64 位的 chunk handle 唯一标识，读写操作都是基于 chunk handle 进行的；chunk 保存在普通的 linux 文件系统上，由 chunkserver 管理，并且基于高可用考虑每个 chunk 会有复制的备份保存在多个 chunkserver 上；&lt;/li&gt;
&lt;li&gt;ChunkServer：每个数据服务器上都会部署 chunkserver 来负责对文件的 chunk 进行维护，所以一个 GFS 集群中会有大量的 chunkserver；chunkserver 会与 Master 进行心跳交互，向 Master 汇报维护的 chunk 列表及信息，并且根据 Master 的指令进行 chunk 的迁移复制等操作；chunkserver 直接和客户端交互传输和接收文件数据，大量的 chunkserver 可以实现很大的吞吐量；&lt;/li&gt;
&lt;li&gt;Client：应用代码会链接上 GFS 的客户端库，通过库提供的文件操作 API 和 GFS 进行读写操作交互，客户端和 Master 交互获取和执行元数据相关操作，然后直接和 chunkserver 进行文件数据传输，GFS 库不提供 POXISX API 支持文件操作，不会接入到 linux 的 vnode 层；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上面所有的节点组件都是运行在廉价的商用服务器上，不会依赖特别高性能或者特殊的硬件。&lt;/p&gt;
&lt;h4&gt;3.2.4 单节点 Master&lt;/h4&gt;
&lt;p&gt;从上节的架构中，我们可以注意到，在 GFS 集群中，Master 是单节点部署的。在 GFS 中，单节点 Master 的设计极大简化了整个系统的复杂度，让 Master 可以基于全局完整的信息对文件的 chunk 位置和复制等操作进行策略决定。&lt;/p&gt;
&lt;p&gt;另外一方面，GFS 必须对读写等操作交互仔细考虑设计，尽量降低操作和 Master 的相关性，避免单点的 Master 成为整个系统的瓶颈。具体设计上，客户端只会向 Master 请求获取操作相关的 chunkserver 信息，并且将获取到的数据缓存下来作为后续操作的依据，然后直接和 chunkserver 进行文件数据读写交互。文件数据永远不会经过 Master 节点。&lt;/p&gt;
&lt;p&gt;一个简单的读流程如下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先，根据 chunk 的大小值，客户端将文件名和要读取的字节位置 offset 信息转换为文件中的 chunk 索引位置；&lt;/li&gt;
&lt;li&gt;客户端发生一个包含了文件名和 chunk 索引位置信息的读请求到 Master 节点；&lt;/li&gt;
&lt;li&gt;Master 节点给客户端回复该文件的 chunk 对应的 chunk handle 值和 chunk 所在的 chunkserver 列表信息，这里应该包含 chunk 的复制数据位置；&lt;/li&gt;
&lt;li&gt;客户端根据文件名和 chunk 索引缓存 Master 返回的信息；&lt;/li&gt;
&lt;li&gt;客户端直接请求该 chunk 复制数据所在 chunkervser 节点中的一个，提供 chunk 的 handle 值和要读取的字节范围信息，接下来的读操作是由 chunkserver 和客户端进行交互，不再需要 Master 参与；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;实现上，客户端选择 chunk 的 chunkserver 节点原则上是尽量选择最近的一个，而谷歌内部集群的服务器 IP 地址是经过精心编排的，可以根据 IP 地址来计算哪个节点是最近的。此外，在应用中，常见的方式是客户端向 Master 请求多个 chunk 的位置信息，缓存到本地，后续再读取文件时，都可以直接和 chunkserver 交互，大大减少了 Master 的压力。&lt;/p&gt;
&lt;h4&gt;3.2.5 Chunk 大小&lt;/h4&gt;
&lt;p&gt;chunk 文件的大小是 GFS 中一个关键的设计点，通常是采用 64 MB ，比一般的文件系统块大小要大得多。每个 chunk 都是以普通 linux 文件的方式保存在服务器上。GFS 的每个chunk 在创建时不会一下子分配 64MB 的文件，而是采用了延迟分配的方式，只有在实际写数据的时候才按需进行分片。这样可以避免空间浪费和文件碎片。&lt;/p&gt;
&lt;p&gt;一个大的 chunk 文件大小值有几个好处：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;减少了客户端和 Master 的交互，因为读写都是可以基于客户端向 Master 初始请求拿到的 chunk 相关信息缓存数据进行；&lt;/li&gt;
&lt;li&gt;因为 chunk 比较大，一段时间内客户端和 chunkserver 的交互都是对于同一个 chunk 进行操作，这样实现上可以采用持久的 TCP 连接，降低网络相关的开销；&lt;/li&gt;
&lt;li&gt;减少了 Master 需要保存的元数据数量，实现上 Master 会将元数据保存到内存中，整体数据的大小很关键；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;同时这个设计也存在不好的地方，比如对于一个只包含几个 chunk 的小文件，如果过多的客户端同时从这个小文件读取数据，有可能会成为系统中的一个热点文件，相关应用的性能会出现问题。&lt;/p&gt;
&lt;p&gt;在 GFS 的使用中，这种情况曾经出现过，在一个批量队列的系统应用中，一个程序往 GFS 写入一个单 chunk 的文件，然后触发数百个服务器同时执行读操作，导致保存这个文件的 chunkserver 严重超负载，影响到了这些 chunkserver 上其他文件 chunk 数据的读写操作。简单的一个处理方案是对于这种类型的应用，在执行写操作时，提高文件的复制系数，把文件分散到足够多的 chunkserver 中，降低单个 chunkserver 的可能负载。&lt;/p&gt;
&lt;h4&gt;3.2.6 元数据&lt;/h4&gt;
&lt;p&gt;Master 主要是保存以下三种元数据：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;文件和 chunk 的命名空间，也就是目录结构及文件路径信息；&lt;/li&gt;
&lt;li&gt;文件到 chunk 的映射关系，一个文件可以由多个 chunk 组成；&lt;/li&gt;
&lt;li&gt;每个 chunk 的每个副本的位置信息，也就是保存在哪个 chunkserve 上；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于命名空间和文件到 chunk 的映射信息，Master 会保存相关的修改操作本地磁盘的操作日志文件上持久化存储，并且还会复制到一个远程备用的服务器上。这种方式可以简单地保证 Master 崩溃时相关操作和数据的一致性和可靠性。至于 chunk 的位置信息，Master 不会持久化保存下来，只会在每次启动时或者一个 chunkserver 加入到集群时，直接和每个 chunkserver 节点交互获取相关的信息。&lt;/p&gt;
&lt;p&gt;下面将会对 Master 元数据的关键设计点进行说明。&lt;/p&gt;
&lt;h5&gt;3.2.6.1 内存数据结构&lt;/h5&gt;
&lt;p&gt;Master 将数据保存到内存中有几个好处:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;内存访问速度快，也就意味着 Master 节点处理请求操作的速度很快；&lt;/li&gt;
&lt;li&gt;Master 可以方便地定期扫描整个 GFS 集群的状态，根据状态执行一系列的后台处理；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Master 节点的后台处理是 GFS 集群中很重要的内容，对于 GFS 整体集群的数据一致性和高可用等等保证是关键的处理，包含以下内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;垃圾回收：处理异常导致的无效的 chunk；&lt;/li&gt;
&lt;li&gt;重新复制：对于一个 chunk ，如果有副本所在的 chunkserver 服务器挂掉导致复制系数达不到设定的值，需要选择新的 chunkserver 节点重新复制数据，以保证 chunk 数据的高可用；&lt;/li&gt;
&lt;li&gt;基于服务器的负载和磁盘空间信息将 chunk 数据在不同的 chunkserver 之间进行迁移，保持整个 GFS 集群的稳定和高效利用；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以上处理在实现上存在比较多的细节和考虑，论文下面的章节中都有具体地进行了讨论。&lt;/p&gt;
&lt;p&gt;对于把元数据保存到内存中这个方案，一个常见的关注点是整个 GFS 集群的文件容量是受限于 Master 节点服务器的内存大小的。论文中给出了几点解释，一个是每个 64MB chunk 的元数据不会超过 64 字节，另外真的出现容量问题，在 Master 节点服务器上添加内存的代价是比较低的。相对于可能存在的容量限制和需要硬件更新的代价，完全内存数据结构带来的简单、高效、可靠、性能和灵活性是非常值得的。&lt;/p&gt;
&lt;p&gt;PS. 在谷歌应用 GFS 的过程中，容量最终成为了一个不可忽略的问题，根据一个与 GFS 开发工程师的&lt;a href="https://queue.acm.org/detail.cfm?id=1594206"&gt;访谈记录&lt;/a&gt; ，随着谷歌内部的数据量从数百 T 到 PB，甚至到数十 PB 级别，单节点 Master 的确成为了系统的瓶颈。所以后续 GFS 也实现了类似多 Master 的模式。这个访谈记录透露了关于 GFS 的不少有趣内幕，可以看看。&lt;/p&gt;
&lt;h5&gt;3.2.6.2 Chunk 位置&lt;/h5&gt;
&lt;p&gt;对于每个 chunk 数据及其复制在哪个 chunkserver 服务的位置信息，Master 虽然会在内存中保存完整的数据，但是不会将这部分数据持久化到磁盘上。Master 节点启动的时候，直接从每个 chunkserver 拉取所有 chunk 的信息，然后后续定期通过心跳消息维持数据的时效性。这个设计有几点考虑：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实现起来比较简单，不需要考虑太多 chunk 位置信息在 Master 内存、磁盘及 chunkserver 状态变化时的同步处理；&lt;/li&gt;
&lt;li&gt;此外，chunkserver 是保存 chunk 数据的服务，应当有最准确的 chunk 信息，没必要在 Master 还保持一份持久化信息，chunkserver 的失败是很常见的，数据持久化了再处理同步就很麻烦；&lt;/li&gt;
&lt;/ol&gt;
&lt;h5&gt;3.2.6.3 操作日志&lt;/h5&gt;
&lt;p&gt;操作日志是 GFS Master 中很重要的一类数据，关系到 GFS 关键元数据的完整性和可靠性。此外，对于 GFS 上的并发操作，操作日志也天然存在一个逻辑顺序的时间线。对于元数据操作日志， GFS 有以下处理：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;用户的操作产生的元数据只有在成功持久化到磁盘后才会对客户端可见；&lt;/li&gt;
&lt;li&gt;操作日志会被复制备份到多个远程服务器上，并且要在 Master 本地和远程复制节点都把操作日志持久化到磁盘后才会返回响应给用户；&lt;/li&gt;
&lt;li&gt;Master 会将操作日志批量写到磁盘上；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Master 重新启动时，会读取操作日志进行执行来恢复元数据的内存状态。为了减少加载时间，GFS 支持定时将 Master 内存状态数据作为 checkpoint 数据保存到磁盘上。checkpoint 是元数据按内存数据结构直接保存的，在重启时可以直接加载到内存中恢复数据状态，不再需要解析和执行操作。加载完最新的 checkpoint 数据后，Master 只需要将最新 checkpoint 后的操作日志进行重新执行就可以把整体状态恢复到最新了。创建 checkpoint 之前的操作日志可以直接删除掉以减少空间。&lt;/p&gt;
&lt;p&gt;checkpoint 数据的创建也有需要注意的地方，在元数据比较多的时候，整个操作可能是比较耗时的，应该尽量避免对 GFS 正常的业务处理造成影响。所以实现上，Master 会在分离的线程上执行相关的创建操作，此外也会切换一个新的日志文件记录新的操作日志。在 checkpoint 创建完毕之后，会持久化到本地和远程磁盘上。&lt;/p&gt;
&lt;p&gt;操作日志是现在一些分布式系统中比较常见的设计方案，有些系统也会将其称为 Binary Log (binlog), Write Ahead Log (WAL) 等等。实现上也是差不多，首先系统存在状态，操作日志记录的是对系统状态的变更操作，包括插入/更新/删除。正常业务和处理多节点复制之前，都需要保证操作日志持久化成功。然后为了加速服务重启加载，通常也是采用定时 checkpoint 整个状态数据的方式，此外，还可以对操作日志进行压缩处理，比如同一个 Key 的多个变更操作，可以压缩为最后一个更新或者删除操作。&lt;/p&gt;
&lt;p&gt;现在常见的分布式系统中，操作日志也多应用于多复制节点间的数据同步处理，实现上也有差不多的考虑。&lt;/p&gt;
&lt;h4&gt;3.2.7 一致性模型&lt;/h4&gt;
&lt;p&gt;GFS 实现的一致性模型是宽松一致性模型，很好地支持了谷歌的大规模分布式应用，同时实现上也是比较简单和高效的。本节内容主要是具体讨论 GFS 提供的一致性保证，以及对于应用的实现上的影响和需要考虑的地方。&lt;/p&gt;
&lt;h5&gt;3.2.7.1 GFS 的保证&lt;/h5&gt;
&lt;p&gt;首先，文件命名空间，也就是文件目录树相关的变更操作是原子性的，实现上是通过 Master 的锁机制和操作日志来保证的。锁机制保证操作的原子性及数据的正确性，而操作日志则是明确了操作的全局执行顺序。&lt;/p&gt;
&lt;p&gt;对于文件的数据变更操作则复杂得多，操作是否成功，是否并发，都需要考虑。首先需要明确对于文件数据的几个概念:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一致(consistent) : 所有客户端从所有的 chunk 数据副本总是能读到同样的数据;&lt;/li&gt;
&lt;li&gt;确定(defined) : 如果对于文件数据的一个变更操作是一致的，并且所有客户端都读到这个写的内容，那么这个文件数据是确定；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;变更操作一致性的几种情况：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;非并发成功的变更操作，所有受影响的文件都是确定的，也就是这个操作是一致的： 所有客户端都可以读到同样的数据，并且看到变更操作的内容；&lt;/li&gt;
&lt;li&gt;并发且执行成功的变更操作，所有受影响的文件是未定义状态，但是是一致的: 所有客户端可以读到同样的数据，但是数据可能不是来源同一个操作，更常见的情况是多个并发的变更操作数据混合在一起；&lt;/li&gt;
&lt;li&gt;失败的操作会导致文件不一致：不同的客户端在不同的时间点可能从不同的复制节点文件读取到不一样的数据；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;变更操作主要包含以下两种:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;写操作: 将数据写入到文件指定位置中；&lt;/li&gt;
&lt;li&gt;追加写操作: 将数据原子地写入到文件的当前位置，由 GFS 决定和解决并发写的情况；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;成功的变更操作后，GFS 会保证相关的文件所有的节点都是确定的，实现的方式首先是在每个 chunkserver 上对 chunk 的多个变更操作顺序都是保持一致的，另外，还会利用 chunk 的版本号信息来判断 chunk 的副本数据是否过期失效。包含了失效数据的复制节点不会参与变更操作或者读操作，并且会尽快被执行回收处理。&lt;/p&gt;
&lt;p&gt;之前有提到，客户端会缓存 chunk 的位置信息，所以这个缓存是的确有可能会导致客户端读取到已经过期的副本数据。这个情况无法完全避免，但是实现上会尽量缩减存在的时间窗口和影响。首先客户端的缓存数据是有超时的，在超时后会重新从 master 获取和刷新缓存。然后，对于谷歌来说，大部分的应用模式是追加操作，过期的副本节点一般是返回前面的位置信息，当读者重新从 Master 获取 chunk 信息时，会获取到当前的 chunk 位置信息。&lt;/p&gt;
&lt;p&gt;在一个长期运行的 GFS 集群中，即使操作都是成功的，还是会存在组件失败导致数据损坏或者丢失。GFS 主要是通过定期的握手识别失效的 chunkserver 节点，然后通过 checksum 校验检测数据文件是否已经损坏。一个 chunk 的数据只会在 GFS 检测和处理异常之前完全丢失所有复制节点数据的情况下才会丢失，并且也只是丢失，而不是返回错误的数据。&lt;/p&gt;
&lt;h5&gt;3.2.7.2 应用实现的考虑&lt;/h5&gt;
&lt;p&gt;对于基于 GFS 的应用来说，要实现宽松的一致性模型很简单，只需要以下几个方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;优先使用追加操作&lt;/li&gt;
&lt;li&gt;数据检测点(checkpoint)快照&lt;/li&gt;
&lt;li&gt;写数据自校验、自标识&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;论文中对于应用的常见提了两个常见的应用例子和实现上的一些细节：&lt;/p&gt;
&lt;p&gt;writer 完整写入一个文件的内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;完成数据写入后会原子地重命名文件；&lt;/li&gt;
&lt;li&gt;定期 checkpoint 已写入的数据，可能会包含一些应用层的 checksum 信息；&lt;/li&gt;
&lt;li&gt;reader 只会校验和处理到最新 checkpoint 的数据；&lt;/li&gt;
&lt;li&gt;追加操作相对随机写更高效并且对于应用错误有更好的容错性；&lt;/li&gt;
&lt;li&gt;checkpoint 支持 writer 重启时增量处理数据而不需要重新开始；&lt;/li&gt;
&lt;li&gt;checkpoint 可以让 reader 避免处理到已经成功写入但是从应用层面还是不完整的数据；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;多 writer 追加写入到一个文件，作为一种生产者-消费者队列的模式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GFS 数据追加操作的 &lt;code&gt;append-at-lease-once&lt;/code&gt; 语义实现保留每个 writer 的输出；&lt;/li&gt;
&lt;li&gt;每个  writer 写的记录数据都包含额外的信息，比如 checksum ，这样 reader 可以进行校验；&lt;/li&gt;
&lt;li&gt;reader 在读取数据时可以利用 checksum 信息忽略掉额外的 padding 数据和记录数据碎片；&lt;/li&gt;
&lt;li&gt;对于重复的记录数据，可以通过记录的唯一标识信息进行过滤处理，比如 web 文件的唯一信息；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3.3 系统交互&lt;/h3&gt;
&lt;p&gt;GFS 系统设计上考虑尽量减少系统操作交互中涉及到 Master 节点，以降低 Master 的压力。这一节内容主要是描述在 GFS 的数据变更操作、原子记录追加操作、和快照具体实现上，Master，Client 和 chunkserver 是如何交互的。&lt;/p&gt;
&lt;h4&gt;3.3.1 租约和变更顺序&lt;/h4&gt;
&lt;p&gt;在 GFS 中，变更是指会修改一个 chunk 的内容或者元数据的操作，比如写或者追加写操作。每个针对 chunk 的变更操作都会应用到该 chunk 的所有副本上。GFS 主要是利用租约机制来实现保证 chunk 变更操作在多个副本上的一致。首先 Master 会选择一个 chunk 的节点，并且给其分片一个租约，这个节点被成为该 chunk 的 primary 节点。然后对于该 chunk 的所有变更操作，primary 节点会选择分配一个顺序的操作序号，其他的 chunk 副本都会按照 primary 选择的顺序应用变更操作。&lt;/p&gt;
&lt;p&gt;所以全局的变更顺序是由两个内容决定的:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Master 选择分配的租约顺序；&lt;/li&gt;
&lt;li&gt;在一个租约内，primary 节点指定的序列；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;租约机制主要是基于减少 Master 的管理开销而设计的。一个租约初始的超时时长是 60 秒，不过，只要这个 chunk 一直被修改，primary 就可以一直请求 Master 扩展租约。租约扩展的请求和分配是附加在 chunkserver 和 Master 之间的定时心跳消息中实现的。Master 也可以在某个租约过期前主动地取消其有效期，以实现一些需要的处理操作。如果 Master 和 primary 节点之间的通讯丢失，Master 在上个租约过期后可以安全地将租约分配给一个新的副本。&lt;/p&gt;
&lt;p&gt;下面是一个详细的写操作流程，租约在变更操作中有着非常大的作用：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;首先客户端会请求 Master 获取指定 chunk 当前持有租约的 chunkserver 信息和其他所有副本的位置信息，如果暂无节点持有租约，则 Master 会在所有副本中选择一个进行分配；&lt;/li&gt;
&lt;li&gt;Master 响应 chunk 的 Primary 节点标识及其他副本位置信息给客户端，客户端缓存这些信息，在 Primary 节点无法访问或者响应不再持有租约时才需要再次和 Master 节点进行交互；&lt;/li&gt;
&lt;li&gt;客户端将数据推送到所有的副本节点，并且顺序可以随意选择，每个副本节点 chunkserver 会将数据保存到内部的一个 LRU 缓存中，知道数据被使用或者过期。这是将数据流和控制流解耦的方法，这样客户端可以根据副本节点的网络拓扑信息来优化数据流调度，提升整体的性能，而不用受到 Primary 节点的限制；&lt;/li&gt;
&lt;li&gt;一旦所有的副本节点都响应确认接收到数据，客户端会发生一个携带了之前推送的数据标识的写请求到 Primary 节点。Primary 会给这个写请求分配一个顺序的序号，如果存在多个客户端并发的写请求，Primary 节点会选择一个顺序进行分片，然后该写请求的数据将会按序号顺序保存到节点的本地存储中；&lt;/li&gt;
&lt;li&gt;Primary 节点完成写操作序号的分配和本地保存后，将写请求发生给其他的复制节点，并且补充序号信息，每个 Secondary 节点都按照序号顺序保存写数据；&lt;/li&gt;
&lt;li&gt;所有的 Secondary 完成本地保存的操作后响应给 Primary 节点通知已经完成操作；&lt;/li&gt;
&lt;li&gt;Primary 节点响应客户端，如果有任意的副本节点出现错误，这个错误信息也会响应给客户端。这种错误情况一般是 Primary 成功，然后 Secondary 节点的任意子集也成功的情况，这个通常也视为该客户端的请求已经失败，之前被修改的数据会保持在一个不一致的状态。客户端代码会通过重试几次 3 ~ 7 的步骤来处理这种异常，最后会从一开始再尝试整个请求。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果一个应用的写请求数据大小超过一个 chunk 的大小，GFS 客户端会将这个写请求拆分为多个写操作，然后所有的写操作都会按照上面描述的顺序和各个节点进行交互，也有可能和其他客户端的写请求顺序重叠。所以最终文件的数据是很有可能不同客户端的写请求数据分段保存在一起的，不过所有的副本的有效文件数据内容是一致的，因为都是按照 Primary 节点分配的序号进行保存的。&lt;/p&gt;
&lt;h4&gt;3.3.2 数据流&lt;/h4&gt;
&lt;p&gt;在 GFS 中，数据流和控制流是分离的，控制流是从客户端到 Primary 再到各个 Secondary 节点，而数据则是根据具体的网络拓扑信息来流转的。首先，数据是线性地在选择好的一系列 chunkserver  节点中流转的，一台 chunkserver 不会同时把数据发生给多个其他 chunkserver。chunkserver 是基于网络拓扑选择最近的另外一个 chunkserver 发送数据，并且是利用以 pipeline 的模式进行数据转发，也就是每收到一些数据，立即就转发给下一个 chunkserver。chunkserver 之间的距离是可以通过 IP 地址信息来计算的，在服务器集群中，服务器的 IP 是经过特意设计，根据物理位置来进行分配的。&lt;/p&gt;
&lt;p&gt;这种数据流转方式，一来每个 chunkserver 都只发生数据给一个其他 chunkserver ，可以最大地利用服务器之间的带宽，另外以 pipeline 的模式发送数据，可以极大地降低整体的数据推送延迟，每个 chunkserver 不需要等待接收到完整的数据再进行转发。&lt;/p&gt;
&lt;h4&gt;3.3.3 原子追加操作&lt;/h4&gt;
&lt;p&gt;GFS 对于指定位置的并发随机写并不能保证数据的顺序性，文件最终可能会包含来自多个客户端的数据分片。而对于追加写操作，客户端只是提供了具体的数据，但是最终的文件位置则是由 GFS 选择，原子写入后再返回位置信息给客户端。普通的写需要客户端使用类似分布式锁的机制来实现同步，实现上比较重而且性能开销大。
在谷歌的应用中，常见的是追加类型的写操作，一般都是将文件作为多生产者/单消费者的模式应用。追加写的流程和上面描述的类似，只是有一些额外的处理。当客户端推送数据到所有的副本节点后向 Primary 节点发起写请求时，Primary 节点会检查是否写入数据会导致 chunk 超过最大值 (64MB)，如果超过则将 chunk 填充到最大文件大小，并且指示客户端重新在下一个 chunk 发起追加写请求。&lt;/p&gt;
&lt;p&gt;任意副本节点上的写失败都会导致客户端重试写请求，这样会导致同一个 chunk 在不同副本节点上可能会包含了不同的数据，有可能是同样数据全部或部分重复保存。GFS 只保证数据在一个原子操作中至少写入一次，并且只要写操作是成功的，对于同一个 chunk 的数据，所以复制节点都是写在同样的位置上。&lt;/p&gt;
&lt;h4&gt;3.3.4 快照&lt;/h4&gt;
&lt;p&gt;快照是针对文件或者目录树的，用于备份和数据回滚，实现上基于常见的写时拷贝 (copy-on-write) 技术。当 Master 节点接收到一个创建快照的请求时，会给相关的 chunk 分片一个新的租约，这样客户端进行写操作时就必须再与 Master 节点交互获取最新的 Primary 节点信息，这样 Master 有机会对已创建快照的 chunk 数据执行拷贝处理。&lt;/p&gt;
&lt;p&gt;租约被废除或者过期后， Master 会记录这个快照操作日志到本地磁盘，然后再同步到内存的元数据状态中，复制记录一个快照涉及到的文件和 chunk 等元数据。快照创建后，当一个客户端尝试写到一个之前快照操作相关的 chunk 时，Master 会注意到这个 chunk 当前有大于 1 的引用，将会复制原来的数据创建一个新的 chunk ，之后的写操作就是基于新的 chunk 进行的了，和上面描述的流程类似，先选择一个 Primary 节点分配租约，然后进行相关的写流程。&lt;/p&gt;
&lt;h3&gt;3.4 Master 操作&lt;/h3&gt;
&lt;p&gt;Master 节点负责着 GFS 中的命名空间的相关操作，包含 chunk 副本节点的控制&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;决定 chunk 放置的服务器位置&lt;/li&gt;
&lt;li&gt;创建 chunk 和复制数据&lt;/li&gt;
&lt;li&gt;在整个系统层面协调处理，以保证每个 chunk 都能满足需求的复制程度，均衡整体负载&lt;/li&gt;
&lt;li&gt;回收未使用的存储空间&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;3.4.1 命名空间管理及锁定&lt;/h4&gt;
&lt;p&gt;Master 支持并发的操作，对于存在竞态的数据主要是使用锁机制来保证操作恰当的执行顺序。GFS 的文件逻辑上是以一个全路径的查找表形式来实现命名空间的管理的，路径管理到文件具体的元数据信息。而命名空间树以前缀压缩的形式保存在内存中，并且该树上每个节点都存在一个读写锁。&lt;/p&gt;
&lt;p&gt;举个例子，/d1/d2/d3/.../dn/leaf ，这样一个路径文件的操作，需要获取整个路径上所有节点的锁才可以进行，具体锁类型，前缀的节点需要获取到读锁： /d1, /d1/d2, /d1/d2/d3, /d1/d2/d3.../dn, 而文件上 /d1/d2/d3/.../dn/leaf 根据操作类型需要获取到读或者写锁。&lt;/p&gt;
&lt;p&gt;GFS 中的文件组织不存在目录或者类似 inode 的机制，对于子文件的写操作，只需要获取到父目录的读锁就可以避免文件创建时父目录被删除。并且当前的锁机制可以支持同个目录下多个文件的并发更新操作。&lt;/p&gt;
&lt;p&gt;GFS 的锁机制还有两点值得注意：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;读写锁对象是延迟创建的，并且在不使用的时候就立即删掉；&lt;/li&gt;
&lt;li&gt;Master 操作获取锁的时候为了避免死锁是按照一致的顺序进行：首先是按照命名空间树的层级顺序，然后同层级是按照字典顺序获取；&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;3.4.2 副本放置&lt;/h4&gt;
&lt;p&gt;在真实场景中 GFS 集群是高度分布的，具有很高的复杂性：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;数百个 chunkserver (服务器) 节点；&lt;/li&gt;
&lt;li&gt;数百个客户端请求访问 chunkserver ；&lt;/li&gt;
&lt;li&gt;多个服务器机架，机架之间可能需要跨越一个或者多个交换机；&lt;/li&gt;
&lt;li&gt;机架内部的带宽流量要大于机架之间的；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;对于 chunk 副本的调度和放置，GFS 主要是基于以下两个原则进行的：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;最大化数据的可靠性和可用性；&lt;/li&gt;
&lt;li&gt;最大化利用带宽；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以实现上，副本需要分布在多个机架上，这样读写的流量也可能是需要跨机架进行的，具体实现上存在很多的取舍权衡。&lt;/p&gt;
&lt;h4&gt;3.4.3 创建、重新复制、均衡负载&lt;/h4&gt;
&lt;p&gt;GSF 的 chunk 副本创建有三种情况:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;创建 chunk；&lt;/li&gt;
&lt;li&gt;重新复制：机器出现异常，副本数量不满足设置的值；&lt;/li&gt;
&lt;li&gt;负载均衡：需要在服务器之间基于访问和服务器空间均衡 chunk 的副本分布；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;创建chunk 的第一个副本，会考虑以下内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;选择低于平均磁盘空间使用率的 chunkserver 放置新副本，这样随着系统的运行，不同的 chunkserver 服务器的磁盘使用率是趋于相等的；&lt;/li&gt;
&lt;li&gt;尽量限制每个 chunkserver 上最近创建的 chunk 副本数量：因为创建之后很可能接下来就是大量的写请求，特别是追加写的情况，这样可以均衡写的流量；&lt;/li&gt;
&lt;li&gt;基于 chunk 可用性的考虑，同一个 chunk 的副本也尽量要分布到不同的机架上；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;异常发生时可能会让某个 chunk 的副本数量低于用于设置的值，这时候 GFS 集群需要尽快让副本恢复到原来的状态。多个 chunk 的重新复制是存在一个优先级的，基于以下因素考虑：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;chunk 的副本数量距离设置值的差距，差距越大优先级越高；&lt;/li&gt;
&lt;li&gt;在用的文件的 chunk 优先级高于最近被删除的文件的 chunk；&lt;/li&gt;
&lt;li&gt;阻塞用户操作的 chunk 优先级会被提升到最高；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;chunk 重新复制和上面副本创建的考虑一致，此外为了避免副本重新复制影响到正常的其他读写业务，整个集群中的重新复制的克隆操作数量是受限的，基于 chunkserver 和 集群都存在着一个限制值。此外，chunkserver 也会通过限制复制从源 chunkserver 读取的请求数量来限制复制流量。&lt;/p&gt;
&lt;p&gt;GFS 会定期检测和计算 chunkserver 的磁盘使用状态，来进行 chunk 数据的重新负载均衡，尽量维持每个 chunkserver 的磁盘使用率接近平均值。&lt;/p&gt;
&lt;h4&gt;3.4.4 垃圾回收&lt;/h4&gt;
&lt;p&gt;GFS 对文件删除的磁盘空间回收不是立即的，而是延迟执行的，从文件和文件的 chunk 都是一样的处理机制。&lt;/p&gt;
&lt;p&gt;对于应用主动请求删除的文件，Master 会将该文件重命名为一个新的隐藏的名字，并且补充删除的时间戳。在后续定时检查进行垃圾回收时，对于删除时间大于 3 天 (内部可配置) 的文件及其 chunk 数据，才会进行真正的数据删除和磁盘空间释放，相关的元数据也会被清理掉。在文件被真正删除前，数据都是可读可恢复的。&lt;/p&gt;
&lt;p&gt;垃圾回收还存在另外一种情况，每个 chunkserver 都会和 Master 节点维持定时的心跳，心跳中包含了该 chunkserver 维护的 chunk 信息。而 Master 节点会检测并回复在 Master 已经不存在的 chunk 信息，这时候 chunkserver 需要进行对这些在 Master 已经被删除元数据的 chunk 进行删除处理。这种一般是在 Master 执行删除处理时与 chunkserver 的通信失败导致的情况，这些 chunk 也被称为孤儿 chunk。&lt;/p&gt;
&lt;p&gt;延迟删除的机制有几点优点：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;实现简单，并且在一个大型的分布式系统中，因为组件和服务的异常总是会发生的，及时删除不一定是稳定可靠的，要实现稳定可靠，实现上也存在很多额外的处理和开销，延迟删除提供了一种一致的处理，并且一次失败还会在后续的定时执行的处理中再进行重试；&lt;/li&gt;
&lt;li&gt;随机的删除在定时执行检测时可以合并为批量处理，减少了 Master 节点和 chunkserver 之间的通信开销，并且删除时也可以进行批量的 IO 处理；&lt;/li&gt;
&lt;li&gt;Master 可以选择在系统空闲时进行真的的删除操作，这样一方面避免影响了正常的应用业务，另外一方面也重复利用了服务器的资源；&lt;/li&gt;
&lt;li&gt;延迟删除也可以在无意或者恶意删除时恢复数据；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;当然，延迟删除的机制也不一定是适应所有的场景的，并且也有其不足的地方。比如因为磁盘空间是延迟释放的，所以会存在一定的磁盘浪费问题，这样如果部署 GFS 集群的磁盘资源比较有限，也会存在一些问题。GFS 实现上支持对这种场景进行配置调优，可以对一些目录数下的文件 chunk 不进行复制，并且删除时是立即释放空间。&lt;/p&gt;
&lt;p&gt;所有的机制都是取舍权衡，并不存在一个合适所有业务场景的方案，我们能做的只是根据具体的业务常见选择合适的方案，并且在实现上进行取舍。当然，我们也应该认识到方案存在的缺陷和不能解决的场景，在例外业务场景中更新方案或者调整规避。&lt;/p&gt;
&lt;h4&gt;3.4.5 无效副本检测&lt;/h4&gt;
&lt;p&gt;每个 chunk 都维护着一个版本号，每次 Master 给一个 chunk 的副本分配租约时，这个版本号都会增加。所以，对于出现异常不能接收数据更新版本号的 chunk 数据，Master 节点会将其视为已经过期无效，在读写中都会屏蔽掉这些 chunk 副本，并且在定期的垃圾回收中将其删除，释放空间。当然，定期检测 chunk 副本健康度的处理也会给这些 chunk 重新复制创建新的副本。&lt;/p&gt;
&lt;p&gt;此外，Master 返回 chunk 信息给客户端时也会带上这个 chunk 版本号信息，这样客户端就可以在与 chunkserver 交互时提供最新的版本号信息，chunkserver 也可以根据这个版本号信息来决定是否可以执行相关的操作。这样，客户端总是能和有最新数据的 chunkserver 进行交互操作。&lt;/p&gt;
&lt;h3&gt;3.5 容错和诊断&lt;/h3&gt;
&lt;p&gt;论文从一开始就提及了，GFS 是设计运行在一个异常和错误普遍发生的硬件环境中，所以必须要考虑容错处理。此外，判断是否出现了异常，数据是否损坏也是很关键的内容。&lt;/p&gt;
&lt;h4&gt;3.5.1 高可用&lt;/h4&gt;
&lt;p&gt;GSF 的高可用是基于快速恢复和复制来实现的。快速恢复指的是无论 Master 还是 chunkserver 节点，在任何情况下出现异常终止，都会立即启动恢复状态，对于客户端和其他服务节点来说，必须要考虑连接失败和重试。&lt;/p&gt;
&lt;p&gt;而复制则是指的 chunk 的副本数据分布在不同的 chunkserver 服务器上及不同的机架上，用户也可以基于实际应用的可用性需求来设置副本数量。Master 节点会在 chunk 副本数量不足或者存在副本的数据被检测到已经损坏的情况下对现有的 chunk 数据进行复制，创建新的副本。&lt;/p&gt;
&lt;p&gt;至于 Master 节点，主要是基于可靠性的考虑针对其状态数据进行复制，变更操作日志和快照的创建都会复制到多个服务器上。并且对于状态数据的修改，只有在所有的 Master 副本上保存到磁盘上才被视为提交。这样在运行的 Master 节点失败挂掉时，另外一个 Master 的副本就可以立即基于复制的状态数据启动服务请求。&lt;/p&gt;
&lt;p&gt;影子 Master 节点在 Primary Master 节点宕机时也可以作为只读的 Master 节点与客户端，因为通常与 Primary Master 的落后不是特别的大。影子 Master 节点会按照 Primary 节点的顺序应用操作日志修改状态，这样就可以和主 Master 节点保持一致的状态。&lt;/p&gt;
&lt;h4&gt;3.5.2 数据完整性&lt;/h4&gt;
&lt;p&gt;GFS 是通过 checksum 来判断存储的数据是否已经损坏的。每个 chunk 分为 64KB 大小的块，而每个块都有一个 32 位的 checksum，这个数据是和 chunk 的其他元数据一起保存在内存中的。chunkserver 在客户端读取请求数据时，在返回数据之前会进行 checksum 的检查，如果和内存的不一致，则说明数据已经损坏。这时候 chunkserver 一个是响应错误信息给客户端，让客户端再尝试从其他副本所在的 chunkserver 读取数据，另外还会汇报这个信息给 Master，Master 会创建新的副本，然后再删除这个数据损坏的副本。&lt;/p&gt;
&lt;p&gt;checksum 对读操作有一定的影响，实现上客户端会对齐块大小来读取数据，方便校验 checksum。checksum 的计算对于追加写操作则有特别的优化，支持根据接收到的数据增量地进行 checksum 的计算和追加。对于指定位置的随机写则需要在写之前先校验文件覆盖区域的第一个和最后一个块的 checksum 值，这样如果写的范围是部分覆盖了这两个块数据，可以保证不会隐藏了已经损坏的内容。&lt;/p&gt;
&lt;p&gt;chunkserver  也会在空闲的时候扫描和确认每个 chunk 的每个块的 checksum 值，这样也可以清理一些不太常用的 chunk 已经损坏的数据文件。&lt;/p&gt;
&lt;h4&gt;3.5.3 诊断工具&lt;/h4&gt;
&lt;p&gt;GSF 主要是基于日志进行诊断的，日志包含管家的事件，比如 chunkserver 的上下线信息，并且还包含了所有请求及响应信息，但是不会记录具体的文件数据。基于降低对主业务影响的考虑，日志的写是异步的，并且是顺序的磁盘写操作。&lt;/p&gt;
&lt;h2&gt;4 总结&lt;/h2&gt;
&lt;p&gt;整篇 GFS 的论文包含了大量的系统实现细节，讨论了一个业界真实应用的大型分布式系统中的方方面面，其中很多设计在现在很多新的开源分布式系统中都可以找到类似的实现。GFS 是基于实际的业务场景来设计，设计上即解决了实际的业务问题和硬件资源问题，同时也兼顾考虑实现上的简易性，最终还可以保持相当的健壮性和可靠性。&lt;/p&gt;
&lt;p&gt;这是一篇值得不断重复阅读的论文。&lt;/p&gt;
&lt;h2&gt;5 后记&lt;/h2&gt;
&lt;p&gt;写这篇论文的阅读这节内容期间因为公司各种出差和事务繁忙，另外一方面也因为论文阅读这节内容一开始是逐句翻译，花费了大量的精力和时间。后面几节内容调整为记录重点内容和用自己的语言重新整理描述论文的实现和关键，整体的速度就快 了很多，而且还可以加强理解和思考。后续的课程论文阅读也计划用这个方式进行。&lt;/p&gt;</content><category term="mit6.824"></category><category term="distributed-system"></category></entry><entry><title>6.824 Lecture 2 RPC and Threads Notes</title><link href="https://blog.tonychow.me/mit6.824-letcture2-notes.html" rel="alternate"></link><published>2021-06-29T00:00:00+08:00</published><updated>2021-06-29T00:00:00+08:00</updated><author><name>tonychow</name></author><id>tag:blog.tonychow.me,2021-06-29:/mit6.824-letcture2-notes.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1 概要&lt;/h2&gt;
&lt;p&gt;本课没有涉及分布式系统方面的内容，主要是针对本课程 Lab 使用的编程语言 Go 进行了一个简单的介绍，然后讨论了一下多线程并发相关内容。最后是对一个 Go 写的多线程爬虫代码进行了解读，关注点在并发处理、竞态、锁、多线程协作这块。&lt;/p&gt;
&lt;p&gt;本课相关的材料:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;课堂 Paper: 本课无论文阅读&lt;/li&gt;
&lt;li&gt;课堂录像: https://www.bilibili.com/video/BV1R7411t71W?p=2&lt;/li&gt;
&lt;li&gt;课堂 Note: https://pdos.csail.mit.edu/6.824/notes/l-rpc.txt&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;2 要点&lt;/h2&gt;
&lt;h3&gt;2.1 Why Go&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Thread (goroutine) 支持&lt;/li&gt;
&lt;li&gt;Lock: 锁机制应对并发执行和竞态&lt;/li&gt;
&lt;li&gt;类型安全&lt;/li&gt;
&lt;li&gt;方便的 RPC 库&lt;/li&gt;
&lt;li&gt;GC 内存安全: 垃圾回收&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Go 要比 C++ 更容易使用，语言更简单直接，不会有特别的语法和特性，也不会有那么多奇怪的错误。&lt;/p&gt;
&lt;h3&gt;2.2 关注并发&lt;/h3&gt;
&lt;p&gt;对于 Go 来说，并发一般情况是多个 goroutine 在同一个地址空间并发执行。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;I/O concurrency: 客户端可以请求多个服务端并发等待响应，服务端处理多个客户端的连接请求，在一个请求进行 IO 操作时可以切换处理另外一个请求的计算；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Parallelism: 多线程利用多核，实际系统中应该尽量利用所有 CPU 的计算力；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Convenience: 后台运行，方便执行处理一些分离的任务；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;不用多线程，可以用异步编程 event-driven 的方式:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;单线程单 loop；&lt;/li&gt;
&lt;li&gt;保存每个状态: 比如请求客户端的状态；&lt;/li&gt;
&lt;li&gt;根据事件来执行切换执行任务；&lt;/li&gt;
&lt;li&gt;单个运行无法充分利用多核 CPU；&lt;/li&gt;
&lt;li&gt;实现相对复杂，使用起来也难以理解&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;相对大量线程的情况会更优秀，比如有上百万的连接，对应上百万个线程来说，事件驱动更好，节省资源，同时还可以减少线程切换带来的性能损耗。实现上通常可以多个线程，每个线程都有个独立的事件循环来执行任务，这样可以利用多核资源。比如 Nginx，是基于多 Worker 线程的事件驱动模型来实现高性能并发处理大量请求的支持。&lt;/p&gt;
&lt;h3&gt;2.3 多线程的挑战&lt;/h3&gt;
&lt;p&gt;共享数据、竞态数据: 多线程访问处理容易出现 bug，并发更新可能会出现问题，机器操作可能不是原子指令&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;需要使用锁来解决这个问题；&lt;/li&gt;
&lt;li&gt;或者避免共享可变数据；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;coordination 多线程协作执行&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;channels&lt;/li&gt;
&lt;li&gt;sync.Cond&lt;/li&gt;
&lt;li&gt;waitGroup&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;死锁&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;锁或者 channel 误用，出现彼此依赖释放或者消费的情况，导致了死锁；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2.4 爬虫示例&lt;/h3&gt;
&lt;p&gt;示例代码主要是实现模拟爬虫处理页面抓取的功能，需要考虑以下内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;一个页面可能还包含了其他的页面 URL&lt;/li&gt;
&lt;li&gt;多个页面可能包含同一个 URL，不应该重复抓取&lt;/li&gt;
&lt;li&gt;多个页面直接包含 URL 可能会构成一个环&lt;/li&gt;
&lt;li&gt;页面抓取应当并发进行，可以加速整个任务的执行&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;课堂上主要是介绍了两个版本的并发抓取爬虫：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;基于锁的并发爬虫&lt;/li&gt;
&lt;li&gt;每个发现的 URL 都创建一个抓取页面的线程&lt;/li&gt;
&lt;li&gt;多个线程之间共享一个 map 数据来记录已经抓取到的页面，避免重复和循环抓取&lt;/li&gt;
&lt;li&gt;多个线程对共享的 map 数据操作时需要加锁，避免出现竞态并发更新/读取，在 Go 这会导致 panic 或者内部数据异常&lt;/li&gt;
&lt;li&gt;可以通过 go 编译器自身的 &lt;code&gt;-race&lt;/code&gt; 工具来检查代码中的竞态问题&lt;/li&gt;
&lt;li&gt;基于 Channel 的并发爬虫&lt;/li&gt;
&lt;li&gt;区分为 Master 和 Worker 线程&lt;/li&gt;
&lt;li&gt;Master 线程创建 Worker 线程去抓取单个页面&lt;/li&gt;
&lt;li&gt;Master 和 Worker 线程之间共享一个 channel，Worker 把抓取到的页面里面包含的 URL 发送到这个 channel；&lt;/li&gt;
&lt;li&gt;Master 记录 Worker 执行抓取过的 URL，从 channel 获取到新的页面，先检查是否已经抓取过，如果没有则启动新的 Worker 线程抓取，有则跳过；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;基于 channel 不需要加锁，是因为记录抓取过页面的 map 数据实际上没有在多个线程中共享，也不存在多线程并发读取更新的情况。但是实际上，channel 数据结构本身在 Go 的实现应该是存在着锁的，这样多个线程每次只有一个线程可以把 URL 发送到 channel 中。&lt;/p&gt;
&lt;h2&gt;3 总结&lt;/h2&gt;
&lt;p&gt;本课内容相对简单，Go 语言对于并发的支持比较好，提供了方便的线程(goroutine) 启动方式，此外还对多线程间的协作提供了包括 channel 、sync 等工具来支持。课程原本是用 C++ 来实现 Lab 相关的编码的，近些年在 Go 语言成熟起来后就切换了。使用 Go 来学习和实现分布式系统，可以让学生更关注分布式系统本身相关的内容，而不是在 C++ 的语言特性和代码 Bug 中花费大量的时间。&lt;/p&gt;
&lt;p&gt;Go 语言本身也比较适合网络编程，在业界中有不少的成熟的分布式系统实现，比如 etcd、TiDB、Kubernetes 等。&lt;/p&gt;</content><category term="mit6.824"></category><category term="distributed-system"></category></entry><entry><title>6.824 Lecture 1 Introduction Notes &amp; Paper Reading</title><link href="https://blog.tonychow.me/mit6.824-letcture1-notes.html" rel="alternate"></link><published>2021-06-28T00:00:00+08:00</published><updated>2021-06-28T00:00:00+08:00</updated><author><name>tonychow</name></author><id>tag:blog.tonychow.me,2021-06-28:/mit6.824-letcture1-notes.html</id><summary type="html"></summary><content type="html">&lt;h2&gt;1 写在前面&lt;/h2&gt;
&lt;p&gt;&lt;a href="https://pdos.csail.mit.edu/6.824/index.html"&gt;MIT 6.824 分布式系统&lt;/a&gt; 是一门研究生课程，主要关注的内容是分布式系统相关的抽象及实现技术，包含容错、复制、一致性等主题。方式上是通过研读分布式领域的经典论文，分析和讨论这些论文包含的系统实现来进行学习和理解分布式系统。&lt;/p&gt;
&lt;p&gt;从课程的论文来看，偏向工程实践，包含了业界经典的分布式系统工程论文，比如来自谷歌的 MapReduce、GFS、Spanner，也包含了 Zookeeper、Spark、Memcached 等流行的开源分布式中间件，此外还有 Bitcoin 等相对新的分布式系统。&lt;/p&gt;
&lt;p&gt;课程还包含了 4 个 Lab，引导学生从实现经典的 MapReduce 到实现分布式一致性算法 Raft 来进行容错的复制集群，最终会实现一个基于 Raft 的分片 kv 分布式存储系统。从 Lab 的设计来看，这门课程包含了不少的动手编码内容，有一定的挑战性。&lt;/p&gt;
&lt;p&gt;对于本课程的学习，个人计划是按照课程的 &lt;a href="https://pdos.csail.mit.edu/6.824/schedule.html"&gt;Schedule&lt;/a&gt;  进行，阅读每节课相关的论文或者材料，然后观看每节课的课堂录像，并进行课堂笔记的记录，看完后再对笔记进行整理和补充个人的思考，加上个人对每节课论文的分析和阅读思考补充为每节课的总结文章。最新的课堂录像是 2020 年的课程，发布在 &lt;a href="https://www.youtube.com/watch?v=cQP8WApzIQQ&amp;amp;list=PLrw6a1wE39_tb2fErI4-WkMbsvGQk9_UB"&gt;Youtube&lt;/a&gt; 上，国内可以在 &lt;a href="https://www.bilibili.com/video/BV1R7411t71W"&gt;B 站&lt;/a&gt; 找到。&lt;/p&gt;
&lt;h2&gt;2 概要&lt;/h2&gt;
&lt;p&gt;第一节课主要是对整个课程的介绍，对分布式系统要解决的问题和面临的挑战进行了概括，然后是对本课涉及到的论文 MapReduce 进行了解读。因为课程后面比较多的学生进行了提问，所以本课对 MapReduce 并没有进行完整的讨论，缺失的内容可以参考往年课堂完整的 Note。&lt;/p&gt;
&lt;p&gt;本课相关的材料:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;课堂 Paper - MapReduce: &lt;a href="https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf"&gt;https://pdos.csail.mit.edu/6.824/papers/mapreduce.pdf&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;课堂录像: &lt;a href="https://www.bilibili.com/video/BV1R7411t71W?p=1"&gt;https://www.bilibili.com/video/BV1R7411t71W?p=1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;课堂 Note: &lt;a href="https://pdos.csail.mit.edu/6.824/notes/l01.txt"&gt;https://pdos.csail.mit.edu/6.824/notes/l01.txt&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;3 要点&lt;/h2&gt;
&lt;h3&gt;3.1 为什么需要分布式系统?&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;扩容: 通过并行提升系统的容量，比如多个服务器可以分散处理请求和数据存储，这里容量不同类型系统不一样，包括吞吐量和数据存储等；&lt;/li&gt;
&lt;li&gt;容错: 主要是通过复制来实现容错；&lt;/li&gt;
&lt;li&gt;物理分割: 一些系统为了靠近外部依赖或者服务的其他实体，物理上就存在分布的状态；&lt;/li&gt;
&lt;li&gt;安全隔离: 基于安全性，部分系统需要分布式实现；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3.2 分布式系统面临的挑战&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;并发: 无论是处理请求的并发还是系统内部组件之间的并发交互，对于实现一个可靠分布式系统来说都是必须要去解决的问题；&lt;/li&gt;
&lt;li&gt;局部错误: 单台服务器或者电脑，发生硬件错误可能是一两年的频率，但是对于一个有着上千台服务器的大型集群来说，硬件错误每天都是必然的事件，网络、电源、磁盘，每天都可能会发生错误，一个可靠的分布式系统必须要良好应对硬件错误；&lt;/li&gt;
&lt;li&gt;性能: 系统的性能是否可以随着服务器数量线性提升？这是一种理想状态，实际上是很难实现，而且分布式的系统往往带来了更复杂的情况；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;3.3 基础设施&lt;/h3&gt;
&lt;p&gt;本课程主要关注的是服务端基础设施类的软件系统：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;存储: 从数据存储到更底层的文件系统；&lt;/li&gt;
&lt;li&gt;通讯: 分布式系统组件之间的通讯网络协议；&lt;/li&gt;
&lt;li&gt;计算: 分布式的计算模型；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个大主题: 抽象与简化分布式存储和计算基础设施的接口便于构建应用和对应用隐藏分布式系统的内部复杂性。这是个很困难的事情。&lt;/p&gt;
&lt;h3&gt;3.4 课程主题&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;实现&lt;ul&gt;
&lt;li&gt;RPC: 对调用方隐藏实现是通过不可靠网络通讯得到的结果；&lt;/li&gt;
&lt;li&gt;Threads: 多核、并发，简化实现操作；&lt;/li&gt;
&lt;li&gt;Concurrency Control: 锁，处理竞态，保证正确性；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性能&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;扩展性: 理想状态 → 系统性能可以随着硬件线性增长&lt;/li&gt;
&lt;li&gt;系统性能: 吞吐量、容量;&lt;/li&gt;
&lt;li&gt;一些性能无法通过增加机器数量提升: 单个用户的请求响应时间，所有用户同时更新同一个数据(涉及到数据竞争) ；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;容错&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;单台服务器可以稳定运行很久；&lt;/li&gt;
&lt;li&gt;服务器数量多的时候，错误不是随机或者罕见的事件，而是必然事件，总是会有机器出现问题；&lt;/li&gt;
&lt;li&gt;分布式系统需要考虑容错性才能对应用隐藏系统的内部复杂性；&lt;/li&gt;
&lt;li&gt;系统的可用性 Availability: 在错误发生时总是能对外提供正常的服务 ；&lt;/li&gt;
&lt;li&gt;可恢复性 Recoverability: 无法应对的错误修复后，服务能正常恢复，尽可能保持错误发生前的状态；&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;方案&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;持久存储 、非易失存储→ 硬盘，SSD，记录数据 checkoutpoint，服务恢复后读取数据恢复状态；&lt;/li&gt;
&lt;li&gt;复制集群: 数据复制到多台服务器上；&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;其他主题&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;一致性: 数据、状态 → 多副本情况下，因为缓存或者同步的问题，需要考虑数据的一致性&lt;/li&gt;
&lt;li&gt;强一致性: 保证数据的一致性 → 从所有节点都可以看到最新的数据 → 可用性受影响 → 更多的数据通讯 → 异地，跨大洲的复制节点对强一致性有更大的性能损耗；&lt;/li&gt;
&lt;li&gt;弱一致性: 最终一致；&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;3.5 MapReduce&lt;/h3&gt;
&lt;p&gt;意义和起源&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;大量的数据: 数十 BT；&lt;/li&gt;
&lt;li&gt;数千台服务器；&lt;/li&gt;
&lt;li&gt;分布式的任务需要专家程序员写分布式的代码去分布处理任务；&lt;/li&gt;
&lt;li&gt;需要一个易用的框架，方便实现分布式任务，并对工程师隐藏分布式的复杂；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Map&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;关注输入文件&lt;/li&gt;
&lt;li&gt;将文件分散为多个文件，多个 Map 任务&lt;/li&gt;
&lt;li&gt;输出处理的中间文件 → k/v&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Reduce&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;收集 Map 任务产生的中间文件&lt;/li&gt;
&lt;li&gt;按照规则聚合中间文件的数据&lt;/li&gt;
&lt;li&gt;输出聚合结果到最终文件: k → count&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Map 和 Reduce 都是任务，N 个 Map 和 M 个 Reduce 可以分布到多台服务器上执行，可以达到 N 倍的性能提升。&lt;/p&gt;
&lt;p&gt;Word Count: 计算文件中每个单词出现的数量&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Map(k, v): k 是文件名，v 是文件的内容&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bash
Map(k, v)
    split v into words
    for each word w
      emit(w, "1")&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Reduce(k, v): k  是单词，v 是单词列表&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bash
Reduce(k, v)
    emit(len(v))&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;一个真实的 MapReduce 工程是可能存在多个阶段的 Map/Reduce , 构成一个 pipeline 处理流，得到最终需要的结果。&lt;/p&gt;
&lt;p&gt;计算模型特性&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;纯函数无副作用、无外部依赖状态；&lt;/li&gt;
&lt;li&gt;计算模型需要可抽象为 Map/Reduce: 不支持无法抽象为 Map/Reduce 的计算任务；&lt;/li&gt;
&lt;li&gt;网络对当年的 MapReduce 存在很大的限制 → 50 M/s ；&lt;/li&gt;
&lt;li&gt;一些任务可能需要大量的数据复制: 比如排序任务，需要全量的数据进行移动；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;谷歌中的 MapReduce 底层依赖 GFS 。&lt;/p&gt;
&lt;h2&gt;4 Paper&lt;/h2&gt;
&lt;p&gt;本课的论文是来自谷歌 2004 年发表的 MapReduce，这是一个当年在谷歌基础设施中被广泛应用于大数据处理任务的编程框架，工程师只需要定义好 Map 和 Reduce 两种函数，就可以利用实现好的框架库在数千台服务器中并行执行大数据处理任务。这篇论文和 GFS、BigTable 并称为谷歌大数据三大论文， 一起催生了谷歌大数据基础设施的开源版本 Hadoop 。Hadoop 成为今后十多年大数据领域占用绝对地位的基础设施，至今，Hadoop 生态不断发展，依旧是大数据相关业务基础设施的最佳选择。&lt;/p&gt;
&lt;h3&gt;4.1 Map/Reduce 计算模型&lt;/h3&gt;
&lt;p&gt;Map/Reduce 计算模型源自函数编程语言，是用于处理列表类型数据的高阶函数，支持传入一个函数和列表，输出对列表应用传入函数的结果。以 &lt;a href="https://gigamonkeys.com/book/collections.html"&gt;Common Lisp&lt;/a&gt; 为例，Map 函数定义及示例如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt; &lt;span class="nv"&gt;result-type&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="nv"&gt;sequence1&lt;/span&gt; &lt;span class="nv"&gt;sequence2...&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;; 示例&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt; &lt;span class="ss"&gt;&amp;#39;vector&lt;/span&gt; &lt;span class="nf"&gt;#&amp;#39;&lt;/span&gt;&lt;span class="nb"&gt;*&lt;/span&gt; &lt;span class="o"&gt;#(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;#(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="nv"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="o"&gt;#(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt; &lt;span class="mi"&gt;24&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Map 函数接受一个 N 参数的函数和 N 个序列，将 N 个序列同序号元素作为函数的参数，应用后将函数输出结果连接起来作为一个新的序列返回。如以上示例，&lt;code&gt;vector&lt;/code&gt; 是返回的序列类型，&lt;code&gt;*&lt;/code&gt; 是函数，&lt;code&gt;#(1 2 3 4 5)&lt;/code&gt; 和 &lt;code&gt;#(10 9 8 7 6)&lt;/code&gt; 是输入的两个序列，执行结果是两个序列同序号元素应用函数乘 &lt;code&gt;*&lt;/code&gt; 后的结果序列 &lt;code&gt;#(10 18 24 28 30)&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;Reduce 函数定义及示例如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce&lt;/span&gt; &lt;span class="k"&gt;function&lt;/span&gt; &lt;span class="nc"&gt;sequence&lt;/span&gt; &lt;span class="k"&gt;&amp;amp;key&lt;/span&gt; &lt;span class="ss"&gt;:from-end&lt;/span&gt; &lt;span class="ss"&gt;:start&lt;/span&gt; &lt;span class="ss"&gt;:end&lt;/span&gt; &lt;span class="ss"&gt;:initial-value&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;; 示例&lt;/span&gt;
&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;reduce&lt;/span&gt; &lt;span class="nf"&gt;#&amp;#39;&lt;/span&gt;&lt;span class="nb"&gt;+&lt;/span&gt; &lt;span class="o"&gt;#(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt; &lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="nv"&gt;==&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;55&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;Reduce 函数接受一个 2 参数的函数和一个序列，首先将该序列的前 2 个元素作为函数参数调用得到一个结果，然后将结果和后一个元素再次作为函数参数调用，依次一直到最后一次函数调用，得到最终的结果。如上示例，函数是加 &lt;code&gt;+&lt;/code&gt;，示例表达式的效果是将序列里面的所有元素相加。&lt;/p&gt;
&lt;h3&gt;4.2 MapReduce 编程模型&lt;/h3&gt;
&lt;p&gt;在本论文中，MapReduce 的编程模型与 Map/Reduce 不太一样，计算任务被抽象为接收和产生 Key/Value 对的 Map 和 Reduce 函数，并且由用户根据计算定义提供给 MapReduce 框架进行执行。Map 函数接受输入的 Key/Value 对，然后产生中间 Key/Value 对，MapReduce 库将中间数据相同 Key 的 Value 数据聚合起来，再交给 Reduce 函数计算，然后输出最终的计算结果 Key/Value 对。简化表达如下:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;map(k1,v1) → list(k2,v2)
reduce(k2,list(v2)) → list(v2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;论文中举了一个计算文件中每个单词出现数量的计算任务:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;code&gt;&lt;span class="nt"&gt;map&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;String&lt;/span&gt; &lt;span class="nt"&gt;key&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;String&lt;/span&gt; &lt;span class="nt"&gt;value&lt;/span&gt;&lt;span class="o"&gt;):&lt;/span&gt; 
  &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="nt"&gt;key&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;document&lt;/span&gt; &lt;span class="nt"&gt;name&lt;/span&gt;
  &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="nt"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;document&lt;/span&gt; &lt;span class="nt"&gt;contents&lt;/span&gt;
  &lt;span class="nt"&gt;for&lt;/span&gt; &lt;span class="nt"&gt;each&lt;/span&gt; &lt;span class="nt"&gt;word&lt;/span&gt; &lt;span class="nt"&gt;w&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt; &lt;span class="nt"&gt;value&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;EmitIntermediate&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;w&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;

&lt;span class="nt"&gt;reduce&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;String&lt;/span&gt; &lt;span class="nt"&gt;key&lt;/span&gt;&lt;span class="o"&gt;,&lt;/span&gt; &lt;span class="nt"&gt;Iterator&lt;/span&gt; &lt;span class="nt"&gt;values&lt;/span&gt;&lt;span class="o"&gt;):&lt;/span&gt; 
  &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="nt"&gt;key&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;word&lt;/span&gt;
  &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="nt"&gt;values&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt; &lt;span class="nt"&gt;a&lt;/span&gt; &lt;span class="nt"&gt;list&lt;/span&gt; &lt;span class="nt"&gt;of&lt;/span&gt; &lt;span class="nt"&gt;counts&lt;/span&gt;
  &lt;span class="nt"&gt;int&lt;/span&gt; &lt;span class="nt"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nt"&gt;0&lt;/span&gt;&lt;span class="o"&gt;;&lt;/span&gt;
  &lt;span class="nt"&gt;for&lt;/span&gt; &lt;span class="nt"&gt;each&lt;/span&gt; &lt;span class="nt"&gt;v&lt;/span&gt; &lt;span class="nt"&gt;in&lt;/span&gt; &lt;span class="nt"&gt;values&lt;/span&gt;&lt;span class="o"&gt;:&lt;/span&gt;
    &lt;span class="nt"&gt;result&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="nt"&gt;ParseInt&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;v&lt;/span&gt;&lt;span class="o"&gt;);&lt;/span&gt;
  &lt;span class="nt"&gt;Emit&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;AsString&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nt"&gt;result&lt;/span&gt;&lt;span class="o"&gt;));&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;map 函数的参数 Key 是文件名，Value 是文件内容，函数体就是对文件内容 value 的每个单词直接输出一个 key/value 对到中间文件，key 是该单词，value 是 "1" 表示该单词出现了一次。&lt;/li&gt;
&lt;li&gt;reduce 函数的参数 Key 是某个单词，Values 是中间文件中该 Key 的所有 value 列表，也就是一堆的 "1" 数据。函数体就是对将 Values 列表的数据转换为 Int 数值，然后加起来，最终输出一个结果。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;从上面的编程模型来看，MapReduce 对于计算任务类型是有一定的要求的，需要能够将计算抽象为 Map 和 Reduce，并且应当是无副作用的。这样，才可以把大量数据的计算任务拆分为并行的处理任务分发到大量的服务器上进行计算。&lt;/p&gt;
&lt;h3&gt;4.3 架构与流程&lt;/h3&gt;
&lt;p&gt;&lt;img alt="mapreduce" src="../images/mapreduce.png"&gt;&lt;/p&gt;
&lt;p&gt;上图是 MapReduce 计算任务整体的一个架构和执行流程，MapReduce 是以一个库的形式存在，用户程序加载 MapReduce 库然后拷贝到整个集群的所有服务器上。在不同的服务器上，程序有 Master 和 Worker 两种运行模式，其中 Master 负责和其他 Worker 程序交互分发 Map 或者 Reduce 任务及记录状态等元数据。而 Worker 则分布在大量的服务器上分别执行用户程序定义的 Map 任务或者 Reduce 任务。&lt;/p&gt;
&lt;p&gt;在一个计算任务启动时，MapReduce 库会将任务数据分割为 M 个小文件，大小一般从 16 M 到 64 M ，这个主要是与底层依赖的 GFS 特性相关。&lt;/p&gt;
&lt;p&gt;Master 主要保存以下元数据:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;每个 Map 或者 Reduce 任务的状态: idle, in-process, completed ；&lt;/li&gt;
&lt;li&gt;每个 Worker 节点的唯一标识；&lt;/li&gt;
&lt;li&gt;每个已完成的 Map 任务，保存其产生的 R 个中间文件的位置和大小，用于分发 Reduce 任务；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Worker 根据被分发的任务类型会有不同的执行:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Map 任务:&lt;ol&gt;
&lt;li&gt;读取输入的文件片段内容，解析得到键值对数据，并且将每组键值对数据传给用户定义的 Map 函数执行，然后将产生的中间结果键值对数据缓存在内存中；&lt;/li&gt;
&lt;li&gt;缓存在内存中的数据将会被定时写到本地磁盘中，并且根据用户定义的分片函数，将数据写到本地磁盘上 R 个文件中，然后再把这些文件的位置信息传回给 Master 节点；&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Reduce 任务:&lt;ol&gt;
&lt;li&gt;启动后会根据传入的中间文件位置，通过远程调用的方式读取 Map 任务的本地文件所有内容，然后将所有键值对数据按照 Key 排序，并且同一个 Key 的数据聚合在一起；&lt;/li&gt;
&lt;li&gt;聚合好的 Key 和 Value 列表将会被传给用户定义的 Reduce 函数执行，结果将会被追加写到这个 Reduce 任务的最终输出文件；&lt;/li&gt;
&lt;li&gt;当中间数据文件内容过大内存无法容纳时，将会采用外排的方式进行处理；&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4.4 容错机制&lt;/h3&gt;
&lt;p&gt;MapReduce 在谷歌中是设计来运行在成百上千的普通服务器中处理大量的数据的，错误是必然会发生的事情，MapReduce 需要有相关的容错机制来应对各种可能发生的错误。论文中提及了几种错误情况的应对，主要是从整个 MapReduce 中的各个角色来进行讨论的。&lt;/p&gt;
&lt;h4&gt;4.4.1 Worker 失效&lt;/h4&gt;
&lt;p&gt;Worker 失效是由 Master 负责处理的，Master 会定时 ping 每个 Worker 来保持状态。当一个 Worker 超时未响应时，Master 就会将该 Worker 标记为失效状态，并对该 Worker 执行的任务进行如下处理:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;该 Worker 完成的 Map 任务将会被重置为空闲状态，由 Master 再调度其他 Worker 执行；&lt;/li&gt;
&lt;li&gt;该 Worker 进行中的 Map 任务和 Reduce 任务也会被重置为空闲状态；&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;已完成的 Map 任务需要重新执行是因为考虑到 Map 任务产生的中间文件是保存在 Worker 本地磁盘上的，所以如果该 Worker 失效，并不能确保数据还是有效的。而已完成的 Reduce 任务不重新执行是因为 Reduce 任务的输出结果是保存在全局的文件系统 (GFS) 中的，所以不需要再重新执行。&lt;/p&gt;
&lt;h4&gt;4.4.2 Master 失效&lt;/h4&gt;
&lt;p&gt;MapReduce 中 Master 是单点的，对于 Master 失效的容错处理方案，论文中采用的是定时将 Master 节点的数据写下来，在 Master 挂掉之后，启动一个新的 Master 节点，然后读取上个检测点数据恢复服务。&lt;/p&gt;
&lt;h4&gt;4.4.3 失效处理机制&lt;/h4&gt;
&lt;p&gt;当用户提供的 Map 和 Reduce 函数是确定性函数时，MapReduce 框架需要保证重复执行时，函数的输出都是一致的，就好像整个程序没有发生错误一样。在 MapReduce 中，主要是通过对 Map 和 Reduce 任务的输出内容进行原子提交来实现这个特性。&lt;/p&gt;
&lt;p&gt;首先每个进行中的任务都会将其输出写到一个私有的临时文件，Reduce 任务会产生一个这样的文件，而 Map 任务则会产生 R 个，R 与 Reduce 任务数量一致。不同任务完成后的处理不一样:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Map 任务完成后，会将 R 个临时文件的名字发生给 Master ，由 Master 记录下来作为任务的状态数据，已完成的 Map 任务发送的消息将会被 Master 忽略；&lt;/li&gt;
&lt;li&gt;Reduce 任务完成后，Worker 会将临时文件重命名为最终输出的文件名称，对于 Reduce 任务重复执行在多个机器的情况，MapReduce 框架主要是依赖底层的 GFS 来保证文件重命名操作的原子性；&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;4.5 局部性&lt;/h3&gt;
&lt;p&gt;在谷歌当时的计算集群中，网络带宽是一个相对受限的资源，所有数据在计算时都通过网络进行传输会导致网络带宽成为系统的瓶颈。 MapReduce 的优化方案比较巧妙，主要是通过尽量让数据文件和执行任务在同样的机器上，减少需要通过网络传输的数据数量来解决。具体是:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;首先输入数据主要是通过 GFS 管理，大文件会被分割为 64 MB 的小文件，并且每个小文件都会有多个拷贝，通常是 3 拷贝；&lt;/li&gt;
&lt;li&gt;Master 节点会记录每个小文件的位置信息，并且作为调度 Map 任务参考依据，尽量将 Map 任务调度到存储了该文件拷贝数据的服务器上执行；&lt;/li&gt;
&lt;li&gt;如果 Map 任务无法调度到存储了该文件数据服务器上执行，则尝试会将任务调度到一个接近存储了该文件任何一份拷贝数据的服务器上执行，比如同一个网络交换机下的服务器；&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;整体的考虑是，尽量不需要进行数据传输，如果无法达成，则降低数据传输的成本。&lt;/p&gt;
&lt;h3&gt;4.6 任务粒度&lt;/h3&gt;
&lt;p&gt;MapReduce 的一大设计思路是将一个大数据的处理任务分割为大量的小任务，让大量的服务器来并行处理，以达到加速计算的目的，这也是我们在算法中常见的 &lt;code&gt;divide and conquer&lt;/code&gt; 方法。在实践中，任务的粒度也是需要考虑的内容，论文中对此进行了相关的论述。&lt;/p&gt;
&lt;p&gt;以 Map 阶段的数量为 M，Reduce 阶段的数量为 R，理想状态下，M 和 R 的值应该远远大于 Worker 服务器的数量，这样才方便对 Worker 执行任务动态达到均衡的效果，并且在出现 Worker 节点失效的情况下，也可以加速恢复。&lt;/p&gt;
&lt;p&gt;在 MapReduce 中，因为 Map 和 Reduce 任务的状态等信息都存在 Master 节点的内存中，所以实际上根据 Master 节点的硬件资源是存在一个上限的。在谷歌的实践中，通常是 200000 个 Map 任务和 5000 个 Reduce 任务，运行在 2000 台 Worker 服务器上。&lt;/p&gt;
&lt;h3&gt;4.7 任务备份&lt;/h3&gt;
&lt;p&gt;一个完整的 MapReduce 计算任务需要所有切分的 Map 和 Reduce 任务全部完成才结束。在实践中，常见的一个导致 MapReduce 任务执行时间过长的情况是某个机器上执行的一些 Map 或者 Reduce 任务卡住了，导致任务一直无法完成。比如磁盘出现异常的服务器可能会导致磁盘读取性能大幅度下降，影响到了任务的执行。&lt;/p&gt;
&lt;p&gt;MapReduce 中设计了一种任务备份机制来降低这种异常的影响。主要实现是，当 MapReduce 计算操作接近完成时，对于当前还在执行中的 Map/Reduce  任务，Master 节点会对应调度一个备份的任务执行。原始的任务和备份的任务中，只要有一个执行完毕，Master 就会将该任务标记为完成。&lt;/p&gt;
&lt;p&gt;任务备份机制的关键在于开始备份任务重新执行的阈值，这个根据不同的计算任务特性，应该有不同的具体值。此外，考虑到备份执行任务会导致计算资源的使用增加，所以需要在资源增加和计算加速之间取个平衡点。&lt;/p&gt;
&lt;h3&gt;4.8 优化扩展&lt;/h3&gt;
&lt;p&gt;除了以上提及的具体实现之外，MapReduce 同时还存在着一些特殊的优化扩展点，论文中也提及了不少，值得参考。&lt;/p&gt;
&lt;h4&gt;4.8.1 分片函数&lt;/h4&gt;
&lt;p&gt;在使用 MapReduce 时，通常是由用户来指定想要的 Reduce 任务和输出文件数量，数据通过使用一个针对 Map 产生的中间文件的 Key 进行分片的函数来进行分片处理。默认的分片函数是 &lt;code&gt;hash(key) mod R&lt;/code&gt; ，这个函数可以产生相对均衡的分片结果。但是在实际应用中，不同的任务类型可能会对分片有不同的一个实际需求，比如对于 URL 的 Key，通常我们希望同一个 Host 的结果会到同一个文件中。MapReduce 库中提供了一个特殊的分片函数来支持这个特性，比如 &lt;code&gt;hash(Hostname(urlkey)) mod R&lt;/code&gt; 可以满足刚刚提到的那个需求。&lt;/p&gt;
&lt;h4&gt;4.8.2 顺序保证&lt;/h4&gt;
&lt;p&gt;MapReduce 保证在单个分片中，中间内容 Key/Value 对是以 Key 的升序排序处理的。这个顺序保证方便生成每个分片有序的输出文件，方便实现支持高效的随机查找 Key。&lt;/p&gt;
&lt;h4&gt;4.8.3 组合函数&lt;/h4&gt;
&lt;p&gt;在一些场景中，Map 任务产生的中间 Key/Value 数据可能会存在比较大的重复性，比如计算单词出现次数的任务，初始实现是每个单词输出一个 &lt;Word, 1&gt; 的数据，同一个任务对于同一个单词会产生大量这样的键值对数据。而每个键值对数据都需要通过网络传输到单个 Reduce 任务进行处理。&lt;/p&gt;
&lt;p&gt;在 MapReduce 中，对于这种情况，框架支持用户指定一个可选的组合函数来在数据被通过网络发送前对同样的 Key 进行局部的数据合并处理。组合函数是在每个执行 Map 任务的机器上执行，通常代码实现和用户的 Reduce 函数类似，区别在于 MapReduce 对执行结果处理方式。Reduce 函数的输出会写到一个最终输出文件中，而组合函数的输出则是写到一个将要发生给 Reduce 任务的中间文件。&lt;/p&gt;
&lt;h4&gt;4.8.4 输入输出类型&lt;/h4&gt;
&lt;p&gt;MapReduce 中支持不同的输入输出类型，提供了相关 reader 接口来支持从文本到用户自定义的类型。数据不一定来自文件，也可以来自数据库或者其他内存数据，只需要实现对应的 reader 就可以了。输出也类似，有不同的输出类型支持，也支持用户自定义输出类型。&lt;/p&gt;
&lt;h4&gt;4.8.5 副作用&lt;/h4&gt;
&lt;p&gt;有时候用户可能会发现在 Map/Reduce 执行中输出一些临时辅助性的文件比较方便有用，这样 Map/Reduce 操作就是包含副作用的。MapReduce 依赖应用的 Writer 来保证这些副作用的原子性和幂等。通常应用会将内容写到一个临时文件，在完全生成后原子地重命名临时文件。&lt;/p&gt;
&lt;p&gt;MapReduce 对单个任务产生多个输出文件支持二段提交来实现写文件的原子性，所以这种任务需要输出是确定性的，多次执行不会产生变化。&lt;/p&gt;
&lt;h4&gt;4.8.6 跳过坏记录&lt;/h4&gt;
&lt;p&gt;有时候用户的代码中可能会存在 bug 导致任务执行时处理特定的记录会必然崩溃，导致任务无法完成，如果这些 bug 是第三方库导致的也不好直接修复。对于一些可以允许跳过一些记录不对整体计算产生太大影响的任务来说，MapReduce 支持提供一个可选的执行模式，由 MapReduce 检测这些比如导致执行崩溃的记录，然后在下次执行时跳过这些记录继续执行。&lt;/p&gt;
&lt;p&gt;实现上，每个 Worker 进程都会注册一个信号处理捕获内存段异常和 bug 错误信息。在 Worker 执行 Map/Reduce 操作之前，MapReduce 会存储操作参数的序列号到一个全局变量中。当执行出现异常崩溃时，信号处理器会发送这个参数的序列号到 Master 节点。当 Master 发现某个特定的记录出现错误超过一次时，就会在下一次重新执行时指示该记录应该被跳过。&lt;/p&gt;
&lt;h4&gt;4.8.7 本地执行&lt;/h4&gt;
&lt;p&gt;一个跑在数千台服务器上并行执行的 MapReduce 任务是非常难以调试的。为了方便调试，谷歌实现了一个在单台机器上顺序执行任务函数的 MapReduce 库版本，用户可以自行控制特定 Map 任务的执行。用户通过一个特殊的标记启动程序，就可以使用场景的调试和测试工具对任务进行处理。&lt;/p&gt;
&lt;h4&gt;4.8.8 状态信息&lt;/h4&gt;
&lt;p&gt;Master 节点运行了一个内部的 HTTP 服务器，暴露了一系列的状态页面提供给管理员查看。包含以下内容：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;MapReduce 计算的进度: 完成和进行中状态的任务数量、输入数据大小、中间数据的大小，输出数据的大小、处理速率等等；&lt;/li&gt;
&lt;li&gt;到标准错误信息的连接及每个任务输出的标准输出文件；&lt;/li&gt;
&lt;li&gt;失败的 Worker 节点，失败的 Map/Reduce 任务信息；&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;4.8.9 计数器&lt;/h4&gt;
&lt;p&gt;在计算任务执行过程中，基于统计等需求，总是需要对一些事件或者情况发生的次数进行计数处理。MapReduce 中提供了一个计数器的机制，用户可以在用户代码中创建一个 Counter，并在 Map 等任务处理中根据具体业务增加 Counter 值。MapReduce 框架会从 Worker 节点定时把某个任务的 Counter 信息在 ping 响应时汇报给 Master 节点，当一个任务执行完毕时，Master 节点会聚合计数器信息返回给用户代码。同时，Master 节点针对重复执行的任务汇报的计数器信息也会进行过滤处理，避免同个任务多次计数。计数器信息也会展示在 MapReduce 的状态页面上。&lt;/p&gt;
&lt;h2&gt;5 总结&lt;/h2&gt;
&lt;p&gt;本课主要还是针对分布式系统做了一个概括性的介绍，包含了什么是分布式系统，为什么需要分布式系统以及在当前，分布式系统存在那些挑战，我们整个课程关注的是分布式系统中哪些内容。通过本课的学习，基本能对分布式系统的领域及问题有一个初步的了解。&lt;/p&gt;
&lt;p&gt;MapReduce 论文是一篇相对旧的论文，也是一篇非常经典的分布式系统方面的论文。从论文里面，我们可以看到，基于一个简单的模型，加上对问题域的简化，我们可以利用分布式系统来解决一个传统意义上单机非常难以解决的问题。论文中除了系统的架构和模型值得我们去关注之外，整个系统对于容错、恢复处理等的机制，也是很值得我们去参考的。在当前的业界中，这些思想仍然具备很大的价值。&lt;/p&gt;</content><category term="mit6.824"></category><category term="distributed-system"></category><category term="paper"></category></entry></feed>